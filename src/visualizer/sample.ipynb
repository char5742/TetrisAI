{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: CUDA is on\n",
      "└ @ Main.TetrisAICore.AILux c:\\work_space\\programing\\julia\\TetrisAI\\src\\core\\ai\\lux\\Lux.jl:13\n",
      "WARNING: replacing module TetrisAICore.\n"
     ]
    }
   ],
   "source": [
    "using Tetris\n",
    "include(\"../core/TetrisAICore.jl\")\n",
    "import .TetrisAICore: loadmodel, predict, Model,\n",
    " GameState, MoveState, Action, valid_movement, \n",
    " move, put_mino!, generate_minopos, get_node_list,\n",
    "  Node, draw_game, init_screen, get_key_state, mysleep,\n",
    "mino_to_array, AILux, create_model, select_node, vector2array\n",
    "using CUDA\n",
    "using JLD2\n",
    "using HTTP\n",
    "using CodecZstd\n",
    "using Serialization\n",
    "CUDA.math_mode!(CUDA.PEDANTIC_MATH)\n",
    "module TetrisAICore\n",
    "module AILux\n",
    "using CUDA\n",
    "using Lux, LuxCUDA, NNlib, MLUtils, Zygote\n",
    "import Lux: gpu_device, cpu_device\n",
    "gpu = gpu_device()\n",
    "cpu = cpu_device()\n",
    "export gpu, cpu\n",
    "using JLD2, Optimisers\n",
    "using Statistics, Random\n",
    "using NamedTupleTools\n",
    "include(\"play/layers.jl\")\n",
    "include(\"play/network.jl\")\n",
    "export QNetwork\n",
    "end\n",
    "using .AILux\n",
    "export QNetwork, gpu, cpu\n",
    "end\n",
    "using .TetrisAICore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package Images not found in current path.\n- Run `import Pkg; Pkg.add(\"Images\")` to install the Images package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package Images not found in current path.\n",
      "- Run `import Pkg; Pkg.add(\"Images\")` to install the Images package.\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ Base .\\loading.jl:1766 [inlined]\n",
      "  [2] macro expansion\n",
      "    @ Base .\\lock.jl:267 [inlined]\n",
      "  [3] __require(into::Module, mod::Symbol)\n",
      "    @ Base .\\loading.jl:1747\n",
      "  [4] #invoke_in_world#3\n",
      "    @ Base .\\essentials.jl:921 [inlined]\n",
      "  [5] invoke_in_world\n",
      "    @ Base .\\essentials.jl:918 [inlined]\n",
      "  [6] require(into::Module, mod::Symbol)\n",
      "    @ Base .\\loading.jl:1740\n",
      "  [7] eval\n",
      "    @ .\\boot.jl:383 [inlined]\n",
      "  [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base .\\loading.jl:2070\n",
      "  [9] #invokelatest#2\n",
      "    @ .\\essentials.jl:887 [inlined]\n",
      " [10] invokelatest\n",
      "    @ .\\essentials.jl:884 [inlined]\n",
      " [11] (::VSCodeServer.var\"#202#203\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer c:\\Users\\fshuu\\.vscode-insiders\\extensions\\julialang.language-julia-1.60.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:19\n",
      " [12] withpath(f::VSCodeServer.var\"#202#203\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer c:\\Users\\fshuu\\.vscode-insiders\\extensions\\julialang.language-julia-1.60.2\\scripts\\packages\\VSCodeServer\\src\\repl.jl:274\n",
      " [13] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer c:\\Users\\fshuu\\.vscode-insiders\\extensions\\julialang.language-julia-1.60.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:13\n",
      " [14] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC c:\\Users\\fshuu\\.vscode-insiders\\extensions\\julialang.language-julia-1.60.2\\scripts\\packages\\JSONRPC\\src\\typed.jl:67\n",
      " [15] serve_notebook(pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; crashreporting_pipename::String)\n",
      "    @ VSCodeServer c:\\Users\\fshuu\\.vscode-insiders\\extensions\\julialang.language-julia-1.60.2\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:139\n",
      " [16] top-level scope\n",
      "    @ c:\\Users\\fshuu\\.vscode-insiders\\extensions\\julialang.language-julia-1.60.2\\scripts\\notebook\\notebook.jl:32"
     ]
    }
   ],
   "source": [
    "using Images\n",
    "\n",
    "using FileIO\n",
    "\n",
    "function save_3d_array_as_grid_with_padding(arr::Array{T, 3}, filename::String, padding::Int) where T\n",
    "    h, w, n = size(arr)\n",
    "    normed = (arr .- minimum(arr)) / (maximum(arr) - minimum(arr))\n",
    "    \n",
    "    grid_h = 8\n",
    "    grid_w = 16\n",
    "    if n > grid_h*grid_w\n",
    "        error(\"The 3rd dimension size should not be greater than 64 for an $(grid_h)x$(grid_w) grid.\")\n",
    "    end\n",
    "    \n",
    "    padded_h = h + 2 * padding\n",
    "    padded_w = w + 2 * padding\n",
    "    \n",
    "    composite_img = zeros(Gray{T}, padded_h * grid_h, padded_w * grid_w)\n",
    "    \n",
    "    for i in 1:n\n",
    "        row = div(i-1, grid_w) + 1\n",
    "        col = rem(i-1, grid_w) + 1\n",
    "        slice = normed[:, :, i]\n",
    "        padded_slice = ones(Gray{T}, padded_h, padded_w)\n",
    "        padded_slice[padding+1:padding+h, padding+1:padding+w] .= Gray.(slice)\n",
    "        \n",
    "        composite_img[(row-1)*padded_h+1:row*padded_h, (col-1)*padded_w+1:col*padded_w] .= padded_slice\n",
    "    end\n",
    "    \n",
    "    save(filename, composite_img)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(_QNetwork(), (board_net = (conv1 = (weight = Float32[0.0018217613 -0.0033527242 -0.0031203853; -0.025152232 -0.00464052 0.0032640556; -0.02294185 -0.01400224 -0.025925517;;; 0.010449293 0.060330622 0.05733042; 0.018899068 -0.046915542 -0.028226; -0.013604938 -0.03970599 -0.05542145;;;; -0.018107533 0.053495944 0.0036241466; 0.015072697 0.025318662 0.00078205287; 0.0075143157 0.010056653 0.049457982;;; 0.019937702 0.03000039 -0.042876992; 0.013525487 -0.009810779 0.001252229; -0.008162948 0.024091426 0.022012621;;;; -0.03748307 0.0012654532 -0.008020445; 0.010418658 -0.019869758 -0.015677404; 0.008536812 0.007809981 0.00047863994;;; -0.030878846 -0.058756657 0.030409522; -0.059250776 -0.038361933 0.038822416; 0.0061465455 0.030691138 0.028219905;;;; … ;;;; 0.035518635 -0.011148178 -0.03817483; 0.009181177 -0.01039618 -0.038135964; -0.010724771 0.0061769714 0.03651171;;; 0.016245063 -0.014320265 -0.015950125; 0.044751342 -0.004680996 -0.043016504; 0.053220265 -0.020674208 0.0065800543;;;; 0.011378747 0.06065567 -0.027230104; -0.015436293 0.04124069 -0.0064506987; -0.013888011 -0.024799535 -0.010853816;;; -0.0076384605 0.058327924 -0.002802878; -0.025015336 -0.034285434 0.003617032; 0.027403286 0.033304643 -0.017559756;;;; -0.023689885 0.033779822 -0.03293124; 0.03505056 -0.0029184932 0.016830636; -0.0015959934 0.010806317 0.02004149;;; -0.06341445 0.050476726 -0.050656375; -0.014179927 0.03092544 -0.014709284; 0.03817821 0.02635221 0.0056209746], bias = Float32[-0.0033714694;;; -0.000105461106;;; 0.0025603485;;; … ;;; 4.793429f-5;;; 0.00054888893;;; 0.0001957573;;;;]), norm1 = (scale = Float32[0.99201465, 0.9904626, 0.9972688, 0.98887753, 0.9656499, 0.9784757, 1.0045757, 0.9648841, 0.98594165, 0.9866724  …  0.9317326, 1.0062369, 0.9585434, 1.0332094, 0.99854505, 0.9835973, 1.0017266, 0.95735526, 0.9773095, 1.0054512], bias = Float32[-0.045099813, -0.04044943, -0.007566684, -0.003431244, -0.04512948, -0.048552625, -0.03125922, -0.033781774, -0.014286881, -0.010925499  …  -0.07540608, -0.0006673227, -0.033355407, 0.022603247, -0.018880457, -0.035800055, -0.0035768677, -0.03604041, -0.026401471, -0.007936268]), resblocks = (layer_1 = (resblock = (layer_1 = (weight = Float32[-0.10093089;;; -0.06471048;;; 0.061050266;;; … ;;; -0.059063673;;; -0.020650337;;; 0.10279845;;;; 0.04199903;;; -0.030549185;;; 0.02014421;;; … ;;; -0.013893483;;; -0.02213636;;; 0.0035551593;;;; -0.014618094;;; 0.06915281;;; 0.032456446;;; … ;;; -0.0065975483;;; 0.097272724;;; -0.07226331;;;; … ;;;; 0.06362785;;; 0.06131132;;; 0.06985172;;; … ;;; 0.0020432519;;; -0.01965719;;; -0.007211311;;;; -0.061747413;;; 0.035966;;; 0.049586914;;; … ;;; 0.03700944;;; -0.02209947;;; 0.024072511;;;; -0.016626237;;; 0.06792519;;; 0.05575546;;; … ;;; -0.033359893;;; 0.011511762;;; -3.568903f-5], bias = Float32[-1.3338027f-5;;; -3.3407297f-5;;; 0.00013676548;;; … ;;; 0.00029432916;;; 3.807363f-5;;; -5.4525128f-5;;;;]), layer_2 = (scale = Float32[1.0278385, 0.9544855, 1.0050793, 0.95938003, 1.0174156, 1.0439286, 1.0050282, 1.0744164, 1.0000606, 0.99079394  …  1.0403506, 0.9831268, 0.9378685, 0.97243094, 1.0034208, 1.0028613, 1.055828, 1.0292187, 1.028773, 0.97693133], bias = Float32[-0.0021107995, -0.051258363, -0.007022027, -0.06888131, -0.026672427, 0.0005915673, -0.050692167, -0.06932, -0.030595345, -0.029923268  …  -0.032534737, -0.009502476, -0.06923684, -0.081860684, 0.00401792, -0.037882827, -0.048168734, -0.030054942, 0.015780859, -0.0387068]), layer_3 = NamedTuple(), layer_4 = (weight = Float32[0.025973054 0.0016450896 0.046713244; 0.024738016 -0.030080473 -0.05246267; -0.016983079 0.0525209 0.020614238;;; -0.0828229 -0.043414578 0.018403886; -0.040894393 0.01957242 -0.029743109; -0.017199447 -4.0636147f-5 0.018236052;;; 0.027897527 0.03958446 -0.020786092; -0.031917185 0.020017292 0.0028005473; -0.0038385931 -0.024681734 -0.05662526;;; … ;;; 0.02882297 0.0058558476 0.07877219; -0.028724547 -0.04963192 -0.034705475; -0.05519485 -0.005663971 0.009098166;;; 0.031596836 0.013698216 0.04238608; -0.021046922 0.015369491 -0.072996944; 0.0323365 0.047851816 -0.07485355;;; 0.0015051066 -0.036180735 -0.014509047; 0.0516579 -0.026652949 -0.015953913; 0.018020527 0.047962997 -0.020269098;;;; 0.053320922 -0.017278897 0.0035149683; -0.05177566 -0.024468591 -0.015765242; 0.017494876 -0.021749126 0.017951274;;; 0.0073314947 0.0010016977 0.037839357; 0.025061227 0.0036539715 0.024112761; -0.013286307 0.076620616 0.010909643;;; -0.067167155 0.037983477 0.04018409; 0.05718619 -0.018800046 0.022026405; 0.035385985 0.016336704 -0.07556558;;; … ;;; 0.023150554 -0.02935621 0.08267363; 0.003025196 0.06541002 -0.08956672; -0.06291596 -0.043824367 -0.041186534;;; 0.02753636 0.0035338628 0.027703559; 0.025392719 0.00022095982 -0.042014048; -0.005268865 -0.021104295 -0.008369405;;; 0.0439013 -0.008513367 -0.00024307908; -0.028427597 0.01932246 0.0014228481; 0.038372006 0.03209234 -0.0065905843;;;; -0.0025168285 -0.068412 0.022321614; -0.07164546 -0.011560967 0.0048646103; -0.015881658 0.051503237 0.0555376;;; 0.009155043 0.03464768 0.053647593; -0.032655634 0.047654536 0.010154552; 0.010595273 0.009474778 -0.025369741;;; -0.004557079 0.025647882 -0.0009803601; 0.0062139085 0.021641504 0.05696612; 0.015288283 0.021650482 -0.016343066;;; … ;;; -0.024422806 -0.0043443367 -0.03863163; 0.0115336 0.050240044 -0.03842079; 0.022772973 0.030520521 0.032181375;;; -0.073749304 0.025718799 0.003916664; -0.08670117 0.047990445 -0.019653728; -0.039696377 -0.014693225 -0.019629268;;; 0.031666037 -0.002194734 0.044862725; -0.03226264 0.004876152 0.022981867; 0.011223333 -0.047376823 0.0024859565;;;; … ;;;; 0.042738963 0.015436931 0.056233086; 0.02601195 0.020637516 -0.04894253; -0.029138315 0.011400287 -0.047769904;;; 0.002521639 -0.01874174 0.051988695; 0.04478453 0.031476088 -0.012758087; -0.064466886 -0.034792744 0.009026494;;; -0.025968404 0.01428121 -0.013336369; 0.026153816 -0.0126095 0.0187277; -0.056105934 0.057891037 0.0016394682;;; … ;;; 0.0370653 -0.018951956 0.010340107; -0.058487244 -0.024631798 -0.08561992; -0.05252701 -0.01985312 -0.033463284;;; 0.03494774 -0.05614585 -0.011912483; -0.004534842 -0.036647312 -0.01954962; -0.022703417 -0.009153105 -0.03572085;;; -0.06224104 0.062781624 0.004905446; 0.01519836 0.007757585 -0.029438738; 0.062379055 -0.030543715 0.011991485;;;; -0.0089168465 -0.0018666 -0.05330965; 0.08114252 0.027886428 -0.055134527; 0.07089201 0.003521668 -0.0062701893;;; 0.009828886 0.03417689 0.079706505; 0.031064825 0.03650487 -0.061730925; -0.025696943 -0.045095254 0.04330468;;; 0.022291446 0.03431999 0.0044845967; -0.020288613 0.031162037 0.0047879; -0.051709395 0.0207882 0.026998525;;; … ;;; 0.024625087 0.015476787 -0.044809505; -0.031653065 -0.014248881 0.020558957; 0.049474835 -0.004601251 0.002378358;;; 0.040166788 -0.008713116 -0.0735141; -0.047066737 -0.019015709 -0.014143243; 0.0900312 -0.047706213 -0.06393488;;; -0.045434564 -0.016908726 0.039971743; 0.018498983 -0.0021790648 0.063164376; -0.044735786 -0.011513867 -0.024745315;;;; -0.029183088 -0.034139287 0.022326704; 0.024473766 -0.035010647 -0.051825125; 0.012744555 -0.025679987 0.011165982;;; -0.038771287 0.02514307 -0.032605365; -0.036439624 -0.020073408 0.027539503; 0.05591825 -0.016413739 -0.024144087;;; -0.0612375 -0.055813704 -0.023110442; -0.00858551 0.03245168 0.031735457; -0.0056065675 -0.046276227 0.011459679;;; … ;;; -0.037344377 -0.05373363 0.028662527; -0.04160415 -0.033054184 -0.048421357; -0.056939095 0.020701945 -0.0069819572;;; -0.007119224 -0.049933683 -0.004467294; 0.01693765 -0.05699515 -0.026380064; -0.0025229263 -0.0061645056 -0.027842194;;; 0.041835897 0.04453586 -0.028061133; -0.024714977 -0.016857224 0.010675207; 0.050091192 0.0071235467 0.013294878], bias = Float32[0.00018108392;;; -5.7273017f-5;;; 0.00015087033;;; … ;;; -1.48050685f-5;;; 0.00011684371;;; -0.00019724053;;;;]), layer_5 = (scale = Float32[0.9980236, 1.0240968, 0.9903851, 1.008875, 1.0587112, 1.0115563, 0.9783254, 1.0093768, 1.0123689, 1.0195811  …  1.0250696, 1.0126516, 0.9855358, 0.99526006, 1.0043424, 1.0277808, 1.0149076, 1.0170802, 0.9810504, 0.98989266], bias = Float32[-0.053575207, -0.018160138, -0.068674415, -0.044129293, -0.0058482634, -0.027807618, -0.055850986, -0.03801137, -0.024241665, -0.043289453  …  -0.04437281, -0.04655548, -0.053026095, -0.051118646, -0.05655093, -0.005896455, -0.01651051, -0.044625334, -0.06677679, -0.053647432]), layer_6 = NamedTuple(), layer_7 = (weight = Float32[0.06901547;;; 0.05051371;;; -0.07133294;;; … ;;; -0.06593171;;; -0.082223766;;; -0.04160049;;;; -0.09913244;;; -0.06902706;;; -0.037588455;;; … ;;; -0.040348984;;; -0.031003263;;; -0.07075531;;;; 0.07326244;;; 0.005147316;;; -0.08820929;;; … ;;; 0.049114328;;; 0.053750336;;; 0.020151302;;;; … ;;;; -0.006831379;;; -0.00028614397;;; -0.028876776;;; … ;;; 0.02961757;;; -0.05687319;;; -0.06547056;;;; 0.028868405;;; -0.068593144;;; 0.013614534;;; … ;;; -0.033845793;;; -0.07229105;;; 0.031319775;;;; 0.05874534;;; -0.046276174;;; 0.025673963;;; … ;;; -0.06891143;;; -0.031843882;;; -0.08499354], bias = Float32[-3.8375307f-5;;; 3.5381418f-6;;; 2.55734f-5;;; … ;;; -7.560483f-5;;; 8.0813414f-5;;; 7.773087f-6;;;;]), layer_8 = (scale = Float32[1.0104605, 0.96383464, 1.0232126, 1.0278562, 1.0103155, 0.9954213, 1.0118188, 0.9958594, 0.99124396, 1.0328579  …  1.0461563, 1.0257413, 1.0261464, 1.0272899, 1.0253669, 0.9938368, 0.98818356, 1.0157796, 1.0042323, 1.0018022], bias = Float32[-0.022359716, -0.023268783, -0.005138159, -0.014030046, -0.0045589176, -0.0118565755, -0.02749832, -0.0031504892, -0.014702344, 0.007521299  …  -0.026058435, -0.014215259, -0.010000149, -0.027662873, -0.00042330284, -0.011120854, -0.02545642, -0.028395722, -0.06307191, -0.009350493])), gap = NamedTuple(), conv1 = (weight = Float32[-0.09378;;; -0.101464406;;; -0.009804158;;; … ;;; -0.014842973;;; 0.06048711;;; 0.04804233;;;; -0.08526931;;; 0.043816566;;; 0.059638772;;; … ;;; -0.06385058;;; -0.0355736;;; 0.06338133;;;; -0.019323532;;; 0.03296466;;; 0.041382305;;; … ;;; 0.02399064;;; 0.026086444;;; 0.062195536;;;; … ;;;; 0.0031723585;;; 0.034530677;;; 0.0964474;;; … ;;; -0.10960098;;; -0.1002631;;; 0.026249904;;;; -0.056959312;;; -0.041534208;;; 0.074466944;;; … ;;; 0.028830856;;; 0.03934076;;; 0.052099973;;;; -0.08713165;;; 0.07178431;;; 0.07615102;;; … ;;; -0.04495813;;; -0.06917611;;; 0.0071150293], bias = Float32[-0.040727787;;; 0.0515955;;; -0.03285327;;; … ;;; 0.045644265;;; -0.004934815;;; -0.04659154;;;;]), conv2 = (weight = Float32[-0.036098048;;; -0.045806114;;; -0.086917154;;; … ;;; 0.0076479143;;; -0.035424456;;; 0.080311164;;;; 0.021130675;;; 0.04305636;;; -0.07784851;;; … ;;; 0.031256802;;; -0.120180175;;; 0.065663785;;;; -0.07681769;;; -0.09374537;;; -0.08873563;;; … ;;; 0.009186925;;; -0.044266168;;; -0.039534885;;;; … ;;;; 0.005200408;;; -0.07268507;;; 0.05997392;;; … ;;; -0.004642735;;; 0.026800375;;; 0.084060445;;;; -0.007709526;;; 0.09435565;;; -0.0010477032;;; … ;;; 0.092921115;;; -0.0013424295;;; -0.075359955;;;; -0.0039529344;;; -0.007444842;;; 0.10454242;;; … ;;; -0.06304409;;; -0.07303949;;; 0.06919473], bias = Float32[0.009278386;;; -0.034939338;;; 0.025573188;;; … ;;; 0.017916676;;; 0.0028160883;;; -0.0024268304;;;;])), layer_2 = (resblock = (layer_1 = (weight = Float32[0.023259029;;; -0.030969154;;; -0.08425229;;; … ;;; 0.026591128;;; 0.0007138868;;; -0.0998218;;;; 0.054258846;;; 0.011756539;;; -0.0483292;;; … ;;; -0.08022171;;; 0.04748066;;; 0.007950489;;;; -0.010014157;;; 0.014699028;;; 0.061661262;;; … ;;; 0.073804215;;; 0.044786874;;; 0.02044589;;;; … ;;;; -0.09104002;;; 0.025444815;;; 0.029838901;;; … ;;; 0.056384534;;; -0.010261973;;; 0.06408635;;;; -0.053005874;;; 0.024205813;;; 0.07140452;;; … ;;; 0.06778483;;; 0.040719792;;; 0.01877782;;;; -0.041573886;;; -0.000425083;;; 0.07796175;;; … ;;; 0.021038052;;; -0.06499941;;; -0.066787034], bias = Float32[-0.00013473081;;; 0.00017044538;;; 0.00010749805;;; … ;;; -1.7212604f-5;;; -1.5914273f-5;;; 2.2299946f-5;;;;]), layer_2 = (scale = Float32[1.0495558, 1.0027902, 1.0340936, 1.0073746, 0.98533374, 1.0227729, 1.0051202, 1.0041851, 1.0519364, 1.0354735  …  1.0141844, 1.0399226, 1.0304786, 1.0161066, 1.0120171, 1.0024793, 0.9502307, 1.007168, 1.0101832, 1.0037758], bias = Float32[-0.01087658, -0.053607028, -0.00655998, -0.025599392, -0.051732857, -0.045288615, -0.034993563, -0.023218885, 0.0016839094, -0.048254207  …  -0.00027773555, -0.008072421, -0.043776616, -0.009493322, -0.01979645, -0.053980812, -0.05005522, -0.024947027, -0.043459628, -0.011133534]), layer_3 = NamedTuple(), layer_4 = (weight = Float32[0.035227418 0.050796 0.041903567; 0.0116421105 -0.014162209 -0.00077359495; 0.065134734 -0.020637367 0.05627409;;; -0.018858323 0.027839823 0.008848138; 0.0356294 0.01028362 -0.007697616; -0.0123896925 0.008197102 -0.023148047;;; 0.031896997 -0.064343475 -0.044480283; -0.031206626 0.039773673 -0.0025956975; 0.014483278 0.010121197 0.037077513;;; … ;;; 0.027534554 0.038944375 -0.053581487; 0.060993124 -0.009051939 -0.017094333; -0.027579298 0.05642387 -0.09628333;;; -0.03980069 -0.046777662 -0.009357471; -0.057220653 -0.012023158 -0.027691282; 0.035517592 0.05208828 0.010041932;;; 0.04321518 0.0124067655 -0.0559505; -0.008850425 0.03154089 -0.033295672; -0.018408522 -0.03186297 0.03172392;;;; 0.07088872 -0.022369705 0.0054792827; 0.0078008226 0.00095910794 0.030606829; 0.024750357 -0.037121136 -0.0813249;;; 0.025977422 -0.014641099 0.0108634075; -0.022914598 -0.053238485 -0.011597978; 0.0035817965 -0.0073479326 0.028038051;;; 0.04017151 -0.058060262 -0.0049658758; -0.016465144 0.04281447 -0.038611818; -0.06665743 -0.0068723694 -0.008328653;;; … ;;; -0.0031974905 0.011104033 -0.046037246; -0.05783515 -0.040979136 -0.04124401; -0.033386398 -0.014197797 -0.026836261;;; 0.022668753 0.0020164854 0.013646763; -0.009966367 0.004919183 0.04301369; 0.037904713 0.012879902 -0.0052245427;;; 0.0009836646 -0.011153871 0.014456691; -0.022839071 -0.01996176 -0.005556465; -0.006487036 0.032014515 0.022189965;;;; 0.01592066 0.05535331 -0.00755792; -0.0048155393 -0.0025264819 -0.031180413; -0.046026766 0.0073224516 -0.013375804;;; 0.011634517 0.019701276 -0.025680631; 0.021808065 0.032679666 -0.015726976; -0.020569282 -0.0025361942 0.032307595;;; 0.07062766 0.0015218973 0.02503497; 0.026805667 -0.01765671 -0.057903986; 0.07324835 0.0067002936 0.042201646;;; … ;;; -0.052885402 -0.04745108 -0.020284707; 0.049128592 0.014524049 0.012981233; 0.0194849 -0.0240519 -0.011980827;;; -0.0052276617 -0.04559773 -0.06283553; 0.022675559 0.021620616 -0.06048811; 0.018108428 -0.012321359 -0.060608547;;; -0.061956577 0.013736956 -0.004952246; 0.01839622 -0.037659213 0.015091604; -0.046975862 -0.035585716 0.012502332;;;; … ;;;; 0.0046670213 0.042527225 0.033077076; 0.024930047 0.041672915 0.051838618; -0.00066042936 0.06450031 0.0415569;;; -0.057156034 -0.018568788 -0.0018940348; 0.01181238 0.032161538 0.024861274; -0.03678417 0.0075297244 0.011332741;;; 0.0070746 -0.07063709 -0.06802423; -0.02765406 -0.046184774 -0.009511539; -0.0010756019 -0.03342395 0.0007241648;;; … ;;; -0.04251069 -0.07689336 0.04719044; 0.021377828 0.024920894 0.05749389; 0.0034809564 -0.03200538 0.0037911865;;; -0.05951746 -0.014679039 0.009271316; -0.055883873 0.03557513 -0.0006151483; -0.029700963 0.0035202838 -0.012932228;;; -0.06821128 0.04126245 -0.039151784; 0.010883874 -0.007818395 0.036550887; 0.0058400496 -0.02347967 -0.023053395;;;; -0.059002627 0.030901747 0.078176424; 0.034524024 0.027237728 0.075594224; -0.0033413935 -0.014971635 -0.07904075;;; 0.027609354 -0.01698625 0.021464402; 0.032270443 -0.04065054 -0.009431522; 0.03193868 -0.060148552 0.02827129;;; -0.007198302 -0.016187277 -0.014756722; 0.015749352 -0.052811738 0.004265378; -0.041696668 -0.019950148 -0.023434281;;; … ;;; 0.0027475087 -0.005366434 0.0113397995; 0.034189444 -0.032105394 0.04049964; 0.062375184 -0.07296003 -0.043994486;;; 0.041095074 0.030788628 -0.025322793; 0.008665559 0.06951193 0.022727583; -0.0061118817 0.005829792 0.026706684;;; -0.018478382 -0.013927796 0.0018687071; 0.009117209 0.011668497 0.09438122; 0.040381514 -0.03641094 0.041642588;;;; 0.023612686 -0.03235745 0.009939167; -0.076128304 -0.05152574 0.029903585; -0.07812157 0.024564814 0.008037049;;; 0.010854098 0.0066941236 -0.09198479; 0.037685517 -0.047077928 0.039389495; -0.029493945 -0.020314267 -0.013143585;;; -0.025411427 -0.013360868 -0.014246113; 0.004926 0.037916142 -0.022661615; -0.06793975 0.011460213 0.002156102;;; … ;;; 0.017544212 -0.028787581 0.04509868; -0.06677179 0.043875966 0.029295478; -0.0025078335 -0.034805376 -0.011504246;;; 0.0035305617 -0.017142566 0.012743698; 0.08047765 -0.02573078 -0.01541217; 0.007996424 0.056224298 -0.037127726;;; 0.020092063 -0.047201987 0.010560246; -0.0062617296 -0.0198198 -0.088013984; -0.008927716 -0.038125977 0.009188541], bias = Float32[-1.0495799f-5;;; 7.288297f-5;;; -8.447203f-5;;; … ;;; 2.8366092f-5;;; 8.324215f-5;;; 0.00024215505;;;;]), layer_5 = (scale = Float32[1.0526052, 1.0223508, 1.0215904, 1.01948, 0.9896225, 0.9974738, 1.0138769, 1.0200936, 1.0586157, 1.0220048  …  1.0355479, 1.0163707, 1.0545776, 1.0143515, 1.0321743, 1.0297707, 1.0336294, 0.98037314, 0.99064386, 1.0369854], bias = Float32[-0.007888202, -0.017900575, -0.04090731, -0.0687216, -0.051564377, -0.042939376, -0.01950569, -0.036854077, -0.035520375, -0.03478543  …  -0.028053524, -0.06063875, -0.04565522, -0.061493687, -0.020367907, -0.03199212, -0.033477772, -0.04464726, -0.02410799, -0.03505724]), layer_6 = NamedTuple(), layer_7 = (weight = Float32[0.07883837;;; 0.059940174;;; -0.051230703;;; … ;;; 0.04047843;;; 0.018133814;;; 0.052278645;;;; 0.04337985;;; 0.005674982;;; -0.0139162;;; … ;;; -0.08537328;;; -0.015103491;;; 0.08440177;;;; -0.084563754;;; -0.049282663;;; -0.0016793488;;; … ;;; -0.076383635;;; -0.056329556;;; 0.08334543;;;; … ;;;; 0.04722409;;; 0.014013196;;; -0.083416134;;; … ;;; -0.019009693;;; 0.027601939;;; 0.022328416;;;; -0.041714486;;; -0.073052764;;; 0.0068172226;;; … ;;; -0.049445383;;; 0.043743014;;; -0.015228359;;;; -0.10460583;;; -0.03584437;;; 0.048023313;;; … ;;; -0.0051076887;;; -0.0013657775;;; 0.112990215], bias = Float32[0.00012350018;;; 3.8712325f-5;;; -0.00012344666;;; … ;;; 7.264563f-5;;; -3.3876444f-5;;; 0.00021220879;;;;]), layer_8 = (scale = Float32[0.98769706, 1.0253199, 0.99173206, 1.008384, 1.0320299, 1.0114046, 0.9788708, 1.0305235, 1.0019277, 1.0124562  …  0.9941017, 1.0052897, 0.9914842, 1.0421515, 0.98573405, 1.02081, 0.99385405, 1.0035785, 0.9963526, 1.035079], bias = Float32[-0.0005888487, -0.008962168, -0.02654028, -0.04739004, 0.01539299, -0.03175579, 0.0014271657, 0.015977096, -0.058382295, -0.014163527  …  -0.044698235, 0.00397256, -0.016918367, -0.004937967, -0.03377598, 0.023533225, 0.007537674, -0.030376663, 0.007118774, 0.01315226])), gap = NamedTuple(), conv1 = (weight = Float32[-0.14086092;;; 0.0069813584;;; -0.05290013;;; … ;;; -0.022147676;;; 0.041922517;;; -0.016870653;;;; 0.08061526;;; 0.068004794;;; -0.02020658;;; … ;;; -0.017845996;;; 0.0044046445;;; 0.099273644;;;; -0.07054992;;; 0.055838;;; -0.098383464;;; … ;;; 0.010004879;;; 0.04981109;;; 0.04344957;;;; … ;;;; -0.11944775;;; -0.045616776;;; -0.09091025;;; … ;;; -0.0047009196;;; -0.10675382;;; 0.066538475;;;; -0.020157078;;; -0.06258776;;; 0.10981875;;; … ;;; 0.10919395;;; 0.12594019;;; -0.03196473;;;; -0.073502;;; -0.027209938;;; -0.04984008;;; … ;;; -0.09807636;;; 0.0025961665;;; -0.10432453], bias = Float32[-0.027464218;;; 0.041513253;;; 0.0057948316;;; … ;;; -0.017117677;;; 0.024242453;;; -0.038363524;;;;]), conv2 = (weight = Float32[-0.1070275;;; -0.07765081;;; 0.11274835;;; … ;;; 0.014872766;;; -0.004517261;;; -0.08584332;;;; 0.046112012;;; 0.04824959;;; 0.055429786;;; … ;;; 0.04862324;;; -0.10475459;;; -0.016968712;;;; 0.04335463;;; -0.048723716;;; 0.07090072;;; … ;;; 0.089949705;;; -0.113458395;;; 0.038941234;;;; … ;;;; -0.09357436;;; 0.029271903;;; -0.06401637;;; … ;;; -0.08231237;;; -0.10106947;;; -0.015788408;;;; -0.101722546;;; 0.0958035;;; -0.13343292;;; … ;;; -0.11128677;;; -0.13454965;;; 0.054842804;;;; -0.007564878;;; -0.12701337;;; -0.035872277;;; … ;;; -0.048210606;;; 0.053705607;;; -0.026363201], bias = Float32[-0.011542728;;; 0.01446886;;; -0.010495529;;; … ;;; 0.0022006107;;; -0.007034173;;; 0.029610056;;;;])), layer_3 = (resblock = (layer_1 = (weight = Float32[0.03634949;;; 0.102381475;;; -0.025504973;;; … ;;; 0.08488331;;; -0.04819466;;; -0.0027534673;;;; -0.02048046;;; 0.0050536054;;; -0.009281981;;; … ;;; -0.048667237;;; 0.014578447;;; -0.078864865;;;; -0.07578485;;; 0.019777188;;; -0.07477217;;; … ;;; -0.007005877;;; 0.024338089;;; 0.06814546;;;; … ;;;; 0.0681905;;; 0.03531643;;; 0.0844642;;; … ;;; -0.044406854;;; -0.025408437;;; -0.01685776;;;; 0.008603777;;; -0.038294446;;; 0.071417086;;; … ;;; -0.025732832;;; -0.0474612;;; -0.05547937;;;; 0.015420897;;; -0.047639336;;; -0.0664577;;; … ;;; -0.0050773337;;; 0.08000685;;; -0.07368188], bias = Float32[-8.425543f-5;;; -1.1045579f-5;;; 0.00013994213;;; … ;;; 3.8497943f-5;;; 0.00012258033;;; -3.515779f-5;;;;]), layer_2 = (scale = Float32[1.0395038, 1.0640798, 1.0162157, 0.96008664, 1.0017093, 1.0342143, 1.040375, 1.0235901, 1.0122834, 1.0226834  …  1.0126854, 1.020677, 1.0087364, 0.9766646, 1.0395021, 0.9881882, 0.9969733, 1.0291998, 1.0192176, 1.0015287], bias = Float32[-0.0047835154, -0.038326297, 0.001699779, -0.036857065, -0.052702595, -0.031639177, -0.016889226, -0.029145751, -0.031293593, -0.0011331583  …  -0.04888159, -0.024500191, -0.025171367, -0.058124945, 0.0011558916, -0.07260835, -0.023148937, -0.061932553, -0.03469965, -0.027309777]), layer_3 = NamedTuple(), layer_4 = (weight = Float32[-0.034886606 -0.029634882 -0.07127862; -0.038432702 0.014115805 0.031331398; 0.028032456 0.0085136695 -0.01839579;;; 0.032213338 -0.032125458 -0.069519885; 0.0073027774 0.01684585 0.030071832; -0.0556303 -0.07517964 -0.0049150214;;; 0.018656384 -0.03376332 0.046545718; -0.008526379 0.030530067 0.028293075; 0.02027762 -0.019601287 -0.06805812;;; … ;;; -0.026256545 0.042642344 0.055917084; 0.010982312 0.021491643 0.06598111; 0.022721145 0.014516794 -0.049267437;;; 0.066021435 -0.009638827 -0.05621465; 0.0033301557 0.008318386 -0.01194532; -0.019417986 -0.027324807 0.010677729;;; 0.013811665 -0.03496729 -0.07450673; 0.03370583 0.0256337 -0.03954506; 0.026959546 -0.07209305 0.016056176;;;; 0.005300718 -0.0068954867 -0.101182036; -0.010295166 -0.0060754074 0.030935735; 0.024007402 0.0085537955 0.03087895;;; 0.015752317 -0.037137207 0.011235302; -0.003256859 -0.06960359 0.043415114; 0.005766146 0.05459115 0.01567797;;; -0.021493418 -0.052059002 -0.025483042; -0.010356386 0.030048696 0.017910121; 0.054778725 -0.013685815 0.061103173;;; … ;;; -0.002873178 -0.031700727 0.034838207; 0.01839426 -0.030139163 0.043523803; -0.017763034 0.02050608 -0.05566912;;; -0.018667346 0.027625626 0.0003770315; -0.0438269 0.01645954 0.003317743; 0.057354216 -0.025043007 0.039388303;;; 0.020769313 0.01020618 0.03556756; 0.008499457 -0.018428043 0.02341994; 0.026503462 0.0022500737 0.043065842;;;; -0.043131948 -0.03617469 0.010345333; 0.017266808 -0.045176383 -0.023381237; 0.035345927 -0.033435147 0.02319212;;; -0.010114349 -0.009806185 -0.030382829; -0.047386978 0.036705833 -0.06261551; -0.03925265 0.038056716 0.048267752;;; 0.0057615484 -0.018794714 0.008862948; -0.006362506 0.0106600635 -0.023261864; -0.035506148 0.049343776 0.030405149;;; … ;;; -0.005119483 0.036581878 0.026967831; 0.020775227 -0.05886069 -0.00700234; -0.010317037 0.015847586 0.008774322;;; -0.049646802 -0.04217055 -0.024686713; 0.04940986 0.0023024185 -0.049171403; -0.01855116 0.004406487 -0.03485544;;; 0.055949304 -0.022364091 -0.079973705; 0.022304982 0.047431055 0.03652675; 0.034801386 0.043645594 -0.014905294;;;; … ;;;; -0.008158286 0.005895979 0.016664343; 0.038981512 0.03219981 -0.11403539; -0.027972609 -0.027901996 -0.013575508;;; -0.059664667 -0.01242123 -0.0056992583; 0.003467981 -0.006331674 -0.0658426; -0.0091024 -0.0002078891 -0.054321535;;; -0.011383076 0.05399135 -0.009779926; 0.027946757 -0.05797759 0.028657453; -0.030707784 0.025126895 0.07378685;;; … ;;; 0.0485227 0.0053439084 0.017179513; 0.03751923 -0.051986467 0.019137917; -0.043376856 -0.025967343 0.017059196;;; 0.0025479703 -0.06351714 -0.02439125; -0.056465354 -0.010705206 -0.028316863; 0.0070309555 0.0042767166 0.0020162836;;; 0.06821628 -0.040510792 0.021147462; -0.007876574 0.036134116 -0.06439416; -0.007065427 -0.004231201 0.041975148;;;; -0.07378716 -0.02698298 0.025846664; -0.025725264 0.021770602 -0.029907495; 0.03438892 -0.010248415 -0.012147571;;; 0.007898152 0.031154199 -0.009624982; 0.05088341 -0.004158608 0.042587448; -0.018299721 -0.08363399 -0.02084558;;; -0.039909028 -0.015226114 -0.0036791945; -0.005311568 0.02201669 0.034467857; -0.024585346 0.030895822 -0.016019396;;; … ;;; -0.04556882 -0.013086813 0.06482637; -0.043028567 -0.027187679 0.0146793835; -0.00032399487 0.045399256 0.054531973;;; 0.019280985 -0.021406937 0.015717136; 0.035177268 0.03629878 -0.026046267; 0.003826276 -0.040028065 0.021659313;;; 0.07031261 0.04680981 0.027567768; 0.011912307 -0.039424885 0.054420613; 0.00495448 -0.034235332 0.05328305;;;; 0.052886706 -0.04361949 -0.08740387; 0.053683795 -0.0051766615 -0.017041337; -0.014392498 0.00066845695 -0.0125996405;;; -0.0149098905 0.007917673 -0.039252516; -0.036152747 0.037636574 -0.028947694; -0.029096112 0.010688136 -0.0045082406;;; -0.042550202 0.07296112 -0.023126572; -0.013705397 -0.014670082 -0.02570342; -0.0077026077 -0.019751662 0.041166287;;; … ;;; 0.025501113 -0.06339419 0.040341325; 0.057258606 0.03121195 -0.012189091; -0.06319514 0.0028135972 -0.05281272;;; -0.025123607 0.00279404 0.024637444; 0.004660085 0.06728652 0.030185612; -0.039516326 0.014014689 0.03790478;;; -0.06112331 0.036051713 -0.02418481; 0.06671496 -0.017950295 0.046258736; -0.020861326 -0.019874863 -0.021435227], bias = Float32[6.249557f-5;;; -0.00022809308;;; 0.00013309473;;; … ;;; 1.1401436f-5;;; 9.485343f-6;;; -1.898207f-6;;;;]), layer_5 = (scale = Float32[1.0308031, 0.99996465, 0.99874014, 0.98435515, 1.006755, 1.0154594, 1.0364771, 1.03423, 0.983962, 1.0278575  …  0.97560686, 1.0578591, 1.0269446, 1.0002117, 1.0037874, 1.0276358, 1.0086061, 1.040061, 1.0095013, 1.0055653], bias = Float32[-0.033604875, -0.019308927, -0.055956677, -0.05226609, -0.037653122, -0.029839655, -0.042925213, -0.03760208, -0.0744933, -0.06303329  …  -0.047779784, -0.032352008, -0.06716299, -0.04698618, -0.06323925, -0.017881405, -0.042024627, -0.043564085, -0.06525658, -0.050588947]), layer_6 = NamedTuple(), layer_7 = (weight = Float32[-0.08902189;;; -0.025734603;;; 0.046698563;;; … ;;; -0.069530204;;; -0.07450206;;; -0.0498922;;;; -0.031661578;;; 0.036985118;;; 0.04636253;;; … ;;; 0.059967108;;; 0.094756976;;; 0.022760736;;;; 0.08305957;;; -0.07684327;;; 0.054086406;;; … ;;; -0.011532435;;; 0.056194928;;; 0.09623491;;;; … ;;;; 0.09351387;;; -0.084694035;;; 0.008149341;;; … ;;; -0.04506134;;; 0.0016303709;;; 0.05739912;;;; 0.014360337;;; 0.03638093;;; 0.03768103;;; … ;;; -0.002737369;;; 0.029205767;;; -0.022817563;;;; -0.037868995;;; -0.03291929;;; 0.0717541;;; … ;;; 0.03234427;;; -0.011510991;;; 0.06682739], bias = Float32[0.000106790525;;; -3.286096f-5;;; -1.2003637f-5;;; … ;;; 3.2976917f-5;;; 4.852412f-5;;; 0.00012622299;;;;]), layer_8 = (scale = Float32[0.9943346, 1.0377358, 1.003642, 0.988914, 0.99111104, 1.0120224, 0.99322313, 0.99828494, 1.0246763, 1.0232877  …  1.0102687, 1.0237482, 1.0271448, 0.99995166, 0.9973631, 1.0001128, 1.0679047, 1.0168364, 1.0257899, 1.0161165], bias = Float32[-0.017821036, -0.025010575, -0.018551001, -0.05609581, 0.016627068, -0.07613616, -0.035843372, -0.060984105, -0.025810268, 0.003517356  …  -0.041019797, 0.0207717, 0.007558071, -0.06847332, 0.009029769, -0.018295834, 0.02050062, -0.026684918, -0.034518827, -0.04006783])), gap = NamedTuple(), conv1 = (weight = Float32[-0.06469343;;; -0.056805227;;; 0.023923561;;; … ;;; -0.10121385;;; -0.054587103;;; -0.015684977;;;; -0.06213683;;; -0.035469595;;; 0.0112870345;;; … ;;; 0.079454824;;; -0.0051498394;;; -0.00017271317;;;; 0.032435432;;; 0.1066062;;; 0.0883394;;; … ;;; 0.08976641;;; 0.015136172;;; 0.09461086;;;; … ;;;; -0.08008511;;; 0.060275786;;; -0.0026150723;;; … ;;; -0.023806017;;; 0.020247268;;; 0.07553695;;;; -0.08720554;;; -0.052639116;;; -0.028168645;;; … ;;; -0.041211583;;; 0.01988914;;; 0.043395285;;;; -0.10344284;;; -0.038616884;;; 0.047627173;;; … ;;; 0.039266326;;; -0.026224935;;; 0.10439821], bias = Float32[0.035902735;;; -0.0039753364;;; -0.017983004;;; … ;;; 0.038443305;;; 0.00300962;;; 0.00748341;;;;]), conv2 = (weight = Float32[0.11308992;;; 0.05459728;;; 0.020699805;;; … ;;; 0.060433466;;; -0.0770681;;; 0.029752051;;;; -0.020750998;;; 0.09807317;;; -0.038226213;;; … ;;; 0.06654536;;; -0.06208472;;; 0.009881365;;;; -0.03526784;;; -0.00685981;;; 0.06610466;;; … ;;; 0.11567944;;; -0.07430518;;; 0.06784887;;;; … ;;;; -0.03881475;;; -0.015803449;;; 0.044132184;;; … ;;; 0.035502527;;; 0.012569708;;; 0.06337697;;;; 0.058998443;;; 0.014210229;;; -0.0032000546;;; … ;;; -0.002885815;;; -0.12210037;;; 0.016332079;;;; 0.011541936;;; -0.081895135;;; 0.061332505;;; … ;;; 0.027111104;;; -0.012084109;;; -0.037805825], bias = Float32[-0.0055633583;;; 0.033594634;;; 0.0020870299;;; … ;;; 0.013332016;;; 0.021351604;;; 0.017515957;;;;])), layer_4 = (resblock = (layer_1 = (weight = Float32[-0.07480944;;; -0.04235013;;; -0.03584226;;; … ;;; 0.051223453;;; -0.066594675;;; 0.07871412;;;; 0.046309046;;; -0.08574865;;; -0.017441336;;; … ;;; -0.06139189;;; 0.01814873;;; -0.00632933;;;; -0.04046896;;; 0.017761033;;; 0.075660944;;; … ;;; -0.010208603;;; -0.093291;;; -0.06749384;;;; … ;;;; 0.04606383;;; -0.032632604;;; -0.10172738;;; … ;;; 0.061438806;;; 0.0047787344;;; -0.028574506;;;; 0.084437475;;; -0.00056208234;;; -0.022390446;;; … ;;; -0.04710412;;; -0.075581625;;; -0.007199436;;;; -0.041253787;;; 0.042094484;;; -0.04694423;;; … ;;; -0.095925875;;; 0.06480607;;; -0.062997706], bias = Float32[0.000115967916;;; 3.798485f-5;;; -3.257584f-5;;; … ;;; 0.000101850266;;; -4.5842953f-5;;; -4.03986f-6;;;;]), layer_2 = (scale = Float32[1.0088426, 1.0094563, 1.0172224, 1.0070003, 0.9785744, 0.98557335, 0.9884162, 1.0186207, 1.0152303, 1.0254548  …  1.0036966, 1.0119525, 0.97962546, 0.98815733, 1.0441426, 1.0133103, 0.9941361, 1.0415907, 0.9643106, 0.9919984], bias = Float32[-0.013091254, -0.05452402, -0.035554543, -0.022214258, -0.028609615, -0.077311225, -0.059619784, -0.010197579, -0.035278626, -0.01530252  …  -0.022508731, -0.053266164, -0.072113104, -0.048428413, -0.022403365, -0.01867886, -0.06430892, -0.06144101, -0.031867526, -0.036355067]), layer_3 = NamedTuple(), layer_4 = (weight = Float32[0.061052386 -0.0076666293 0.004879654; 0.061236165 -0.027688071 -0.0062979204; 0.035452593 -0.037292603 -0.020241171;;; -0.035879463 -0.0037088764 0.017227618; -0.06713592 0.017331611 0.03325947; -0.0024704174 -0.022454714 0.00044362765;;; 0.030795846 0.017133538 -0.0071600224; -0.01863937 0.053116255 0.0060508014; -0.019272588 0.028318541 -0.052088294;;; … ;;; 0.0106195565 -0.03593844 0.016272195; 0.007870073 0.00071274454 -0.031735193; -0.004394981 -0.031355795 0.0629137;;; 0.002366076 -0.043660775 -0.05928763; -0.019165201 -0.0097647095 -0.017834231; 0.013968353 -0.037296806 0.021215579;;; -0.054997206 0.047084365 -0.037706915; -0.017366743 -0.023284717 0.02886286; -0.03438471 0.019593941 -0.039493732;;;; 0.002064102 -0.08038339 0.034908064; -0.0650402 -0.027201079 -0.07201456; -0.030973269 0.018044813 0.009445232;;; -0.00012651934 -0.033446252 0.010297602; -0.028484063 0.0074987467 -0.006843353; -0.023666458 -0.02605714 -0.036929723;;; 0.021128833 0.03428294 -0.05461254; 0.022288153 0.00990395 -0.02373839; -0.045993123 -0.07020378 -0.048455473;;; … ;;; 0.026465584 -0.052511685 -0.031702157; 0.0281687 0.04624163 -0.039353725; 0.008798459 0.052486245 0.037960604;;; -0.013647228 -0.005692614 -0.02521121; 0.07134318 0.01292018 0.025687505; 0.038943235 -0.038153246 0.014935492;;; 0.021214085 0.00058384263 0.034702614; -0.019051574 -0.035590477 0.018997384; -0.024285108 0.0036619478 0.017424705;;;; -0.061896477 0.023776323 -0.049457025; -0.009725644 -0.045875955 -0.010257591; 0.03724934 -0.02357918 -0.039162472;;; 0.0142124845 -0.012448356 0.0055514066; 0.0950707 0.06493196 0.045886163; -0.021392914 0.067498475 -0.0044635115;;; 0.005750984 -0.0023694737 0.013117259; -0.005491301 0.0056757824 -0.052212693; 0.0033238593 0.021652767 -0.04763196;;; … ;;; 0.026148738 0.009027927 -0.011828936; -0.020950083 -0.029345606 -0.018864702; -0.008597767 0.042889018 0.033126615;;; -0.02770255 0.021310044 -0.016484426; -0.01591197 -0.013894433 -0.027891079; 0.065495506 0.061470643 -0.00045827343;;; -0.03746121 0.016693994 0.01935683; 0.018925074 -0.05592871 -0.09091502; 0.0125764655 -0.035419982 -0.051056802;;;; … ;;;; -0.0364872 0.020302223 0.009133002; -0.028091507 0.033848137 0.014241134; -0.023007715 0.0023451028 0.019876126;;; 0.053644054 0.039751776 0.018043414; -0.04474907 0.005049694 0.052834094; -0.03355465 -0.04027719 -0.08714797;;; 0.0204287 0.020398315 0.039565142; 0.026425753 -0.029833723 0.03744665; -0.011083552 -0.0032612581 -0.03801021;;; … ;;; 0.0032894085 0.05670307 0.0146965245; -0.03761757 -0.02143319 0.03248786; 0.006537799 -0.058627207 0.03821191;;; -5.1255316f-5 0.019939812 0.061032232; 0.022617111 -0.003409941 -0.027676689; 0.041609116 -0.021249933 0.025137614;;; -0.020806123 0.049316578 -0.028296858; -0.051073257 -0.041611917 -0.014680834; -0.0244891 0.032251894 0.035445288;;;; -0.031400334 -0.04185911 -0.04329034; 0.00479425 0.012603045 -0.012156706; 0.0066130836 -0.0025033967 0.05387688;;; -0.041563224 0.04191484 0.04825172; -0.031293053 0.02994698 -0.0040828674; -0.02032107 -0.0068354188 -0.07046816;;; 0.054007966 0.022602864 -0.01298602; 0.05718473 -0.013353483 0.011128124; -0.01740958 0.0012931254 -0.031960465;;; … ;;; 0.060026307 0.00936051 0.01596067; -0.031003999 -0.040160775 -0.032996867; 0.064696066 0.027857888 -0.0026653593;;; 0.048667483 0.0272568 -0.035696667; 0.006705126 -0.021257078 -0.0059067095; -0.0073255617 0.0081944065 -0.01733606;;; -0.0037483086 -0.02600425 -0.026027968; -0.04113207 0.05905659 0.05976795; 0.035655364 0.007486202 0.021259155;;;; -0.06305339 -0.029519275 -0.025814462; 0.018968081 -0.015920496 0.08122811; 0.017532742 0.03318742 -0.0011764939;;; 0.038399164 -0.03673775 0.0058964193; -0.006108735 -0.0617605 -0.008163251; 0.053307984 0.034860484 -0.026853858;;; 0.044282634 0.0063207527 0.00031186946; 0.040008396 0.015860496 -0.051230486; -0.026645526 -0.01151147 -0.055618163;;; … ;;; 0.05744856 0.006928065 -0.025848383; 0.0023249327 0.0060994434 -0.038318053; -0.027659241 0.014204845 -0.022180142;;; -0.0104169315 0.045164496 0.0389579; 0.008547446 0.028558347 0.056342255; 0.016197488 -0.020087415 -0.0051064906;;; -0.0020794743 -0.051239826 0.0009252965; 0.00074775197 -0.0033628473 -0.0006120065; 0.009671065 0.036537517 -0.0064148307], bias = Float32[0.00014512625;;; 6.089951f-5;;; 5.70565f-6;;; … ;;; -4.1892807f-5;;; 9.7219425f-5;;; 0.00020789096;;;;]), layer_5 = (scale = Float32[0.9963617, 1.0290123, 1.0037217, 0.987624, 1.0225658, 1.0419667, 1.0527334, 0.9998114, 0.99861157, 1.0495414  …  1.0078032, 1.0124043, 1.0204672, 1.0228947, 1.0106767, 1.0314896, 1.0178176, 1.0066706, 1.0291251, 1.0025293], bias = Float32[-0.0637894, -0.025778765, -0.047524735, -0.0427722, -0.022349318, -0.030575188, -0.05872935, 0.014191129, -0.048910365, -0.0286474  …  -0.050801698, -0.049144626, -0.077016234, -0.022097059, -0.056885418, -0.04482358, -0.056449167, -0.026323771, -0.06044785, -0.05286946]), layer_6 = NamedTuple(), layer_7 = (weight = Float32[-0.021914268;;; 0.06991292;;; -3.1882217f-5;;; … ;;; -0.07015443;;; 0.077434346;;; -0.07480204;;;; -0.04832695;;; 0.047616754;;; -0.05864432;;; … ;;; -0.031228993;;; -0.03335536;;; 0.0041044513;;;; -0.06783546;;; 0.023218336;;; 0.073305264;;; … ;;; 0.030758418;;; 0.11469547;;; -0.03998343;;;; … ;;;; 0.09494244;;; -0.040184982;;; 0.021580199;;; … ;;; -0.03665533;;; 0.01606408;;; -0.025173811;;;; -0.0475937;;; 0.06640024;;; 0.09207659;;; … ;;; -0.010768252;;; 0.063481465;;; 0.03798698;;;; 0.04452018;;; 0.097963296;;; 0.06261443;;; … ;;; 0.06924493;;; -0.09781531;;; -0.031858638], bias = Float32[-7.599907f-5;;; -7.692924f-5;;; -0.00011762373;;; … ;;; -0.00012630272;;; -3.9912436f-5;;; -9.091577f-5;;;;]), layer_8 = (scale = Float32[0.98867214, 1.0164113, 0.9962686, 1.010142, 0.9952939, 1.0273364, 1.041657, 0.9917343, 1.0033897, 0.99573827  …  1.0316981, 1.0019821, 1.0177475, 0.97107553, 1.0418056, 1.0322517, 1.0397874, 1.030805, 0.98502487, 1.0002713], bias = Float32[-0.004787539, -0.04435739, -0.027126951, -0.06761238, -0.03567156, -0.03235711, -0.005112001, -0.047378406, -0.027041722, -0.038710106  …  -0.024761824, -0.0074218786, -0.05976206, -0.043822438, -0.012872176, -0.038407996, 0.036324993, -0.037604064, -0.023788592, -0.037945036])), gap = NamedTuple(), conv1 = (weight = Float32[0.09731271;;; -0.083978795;;; 0.03297618;;; … ;;; -0.041668333;;; -0.0081082415;;; 0.023067899;;;; -0.04467205;;; -0.071790144;;; 0.0029236355;;; … ;;; -0.08122336;;; 0.14270906;;; -0.10446606;;;; -0.0023904378;;; -0.00817968;;; 0.030612951;;; … ;;; 0.03639748;;; -0.1090499;;; 0.05649399;;;; … ;;;; -0.124139845;;; 0.0135309035;;; -0.006554194;;; … ;;; 0.09029682;;; 0.020741252;;; 0.04446009;;;; -0.06275686;;; -0.050239716;;; 0.011532186;;; … ;;; -0.07122691;;; 0.0026536551;;; -0.02487517;;;; 0.12634629;;; -0.037982296;;; -0.115890086;;; … ;;; -0.087206736;;; 0.037389193;;; 0.066816285], bias = Float32[0.052563738;;; 0.062203437;;; 0.024152553;;; … ;;; -0.009965214;;; 0.0075301607;;; 0.022512557;;;;]), conv2 = (weight = Float32[-0.06428654;;; -0.080334626;;; 0.04839666;;; … ;;; -0.008476078;;; 0.09179146;;; 0.05603614;;;; 0.07923574;;; -0.027406413;;; -0.04603932;;; … ;;; -0.07783732;;; 0.09199413;;; -0.016421894;;;; -0.059685074;;; -0.030779246;;; -0.07374866;;; … ;;; 0.011397532;;; -0.037358508;;; 0.02348367;;;; … ;;;; -0.009055764;;; 0.11371012;;; -0.12171821;;; … ;;; 0.03735881;;; 0.046385333;;; 0.07309335;;;; -0.053451363;;; 0.0642827;;; -0.037118882;;; … ;;; 0.0634764;;; -0.011864057;;; 0.017542114;;;; 0.12907317;;; -0.08646264;;; -0.13217911;;; … ;;; 0.003941391;;; 0.14769562;;; -0.00280387], bias = Float32[-0.013859326;;; 0.01750057;;; -0.0041110213;;; … ;;; 0.021809854;;; -0.015710391;;; -0.002097131;;;;])), layer_5 = (resblock = (layer_1 = (weight = Float32[0.07081911;;; -0.022576842;;; -0.101557374;;; … ;;; 0.023951536;;; -0.05972242;;; -0.042188883;;;; 0.109845236;;; 0.04873155;;; 0.01365178;;; … ;;; 0.06706253;;; -0.0139746;;; -0.04064821;;;; -0.058336027;;; -0.055517193;;; 0.04520426;;; … ;;; 0.030004673;;; 0.009503713;;; -0.070305824;;;; … ;;;; -0.06745581;;; 0.06775824;;; 0.053089455;;; … ;;; 0.072846964;;; -0.0033310845;;; -0.01956609;;;; 0.048563484;;; 0.0257051;;; -0.084521726;;; … ;;; 0.08561838;;; -0.061065543;;; 0.041707862;;;; 0.004134483;;; -0.062544994;;; -0.022534447;;; … ;;; 0.03329234;;; -0.002157093;;; 0.018704554], bias = Float32[3.8631383f-6;;; 0.00014927257;;; 3.2338914f-5;;; … ;;; 1.730323f-5;;; -4.874817f-6;;; -0.00010883964;;;;]), layer_2 = (scale = Float32[1.0071688, 0.9987364, 0.9949343, 0.9782362, 1.0247201, 1.0038079, 1.014369, 1.0672756, 1.0200645, 0.9831146  …  1.02158, 0.99553716, 1.0196778, 0.9974882, 1.0014927, 0.97796804, 1.0128392, 1.0273948, 1.0302691, 1.0327523], bias = Float32[-0.05038635, -0.02878614, -0.04668846, -0.06944364, -0.028512647, -0.04018399, -0.04456421, 0.012790021, -0.010705181, -0.039092332  …  -0.049039725, -0.026569182, -0.054131944, -0.037835956, -0.07653028, -0.07388442, -0.01460952, 0.002307547, -0.005631446, -0.01886088]), layer_3 = NamedTuple(), layer_4 = (weight = Float32[0.06211446 -0.021289684 0.018956807; 0.017575208 0.011670106 -0.03678941; -0.04212106 -0.0750413 0.028539216;;; -0.005103699 -0.03886581 -0.026804777; -0.014289564 -0.008719045 -0.061450254; -0.022697248 0.03352661 0.043362956;;; -0.05091054 -0.042836003 -0.020672716; -0.0038259935 -0.026712632 -0.06328173; -0.024361506 -0.014152282 0.003134318;;; … ;;; -0.0749478 -0.041026 -0.09053794; 0.034650892 -0.08227274 -0.032326072; 0.029350653 0.020493658 -0.02648476;;; -0.023744153 -0.032522313 -0.0016960887; -0.008044066 -0.043721568 -0.047079023; 0.040637944 -0.0072802836 -0.0093355;;; 0.013897366 0.043755714 0.021579059; -0.0121851405 0.026999032 0.021970686; 0.06285083 0.024341056 0.012852841;;;; 0.051568735 0.076876104 0.011131043; 0.04180973 0.01961077 0.03325701; 0.05720795 -0.026829844 -0.015274987;;; -0.06439335 0.027568722 0.028371489; -0.05003649 -0.025830306 -0.017982082; 0.028186332 0.040895052 -0.057864197;;; 0.068556495 0.04395839 -0.036141213; 0.040418413 -0.03790549 -0.053939924; -0.030775204 -0.04833766 0.04798233;;; … ;;; -0.035440765 0.021408666 -0.041051432; -0.04778578 -0.015215329 0.03939753; 0.010666026 0.037352096 0.03348375;;; -0.085162655 -0.058255758 -0.043282293; 0.030845566 0.011682499 0.05410412; 0.016332494 0.08082668 -0.025052063;;; 0.06363101 0.008069508 0.007911579; 0.031709656 0.012069748 -0.0675555; 0.012616259 0.03500687 -0.025927259;;;; 0.0730855 0.005600955 0.035102423; 0.029467104 -0.0032795328 -0.029028943; -0.007846134 0.06748752 0.014416511;;; -0.0142954225 0.014912384 -0.0012605563; 0.024871616 -0.027759805 0.009191286; -0.010511392 -0.026749032 -0.022980195;;; -0.020989655 -0.03721169 -0.0014402841; -0.016739085 0.0005479302 -0.023186775; 0.058357563 -0.013014442 -0.049393658;;; … ;;; -0.034526218 -0.05369012 -0.03957349; -0.021313712 -0.044371303 -0.042019118; -0.03228168 -0.01138976 0.043583553;;; -0.049273748 -0.016551703 0.005771306; 0.057871733 0.042472757 0.02328339; -0.025636613 -0.038984023 -0.01365889;;; -0.027195632 0.0023485823 -0.016256593; 0.089188255 0.042167097 -0.032344382; 0.007726491 0.02327522 -0.016355885;;;; … ;;;; 0.0072268164 -0.049315806 -0.04189247; -0.055234008 0.014797153 0.039828174; -0.05665123 0.0093702655 0.0049656304;;; 0.032287527 -0.033416055 -0.039671227; -0.01818722 -0.017472433 -0.04734188; -0.047510445 -0.009648242 -0.009345691;;; 0.04592913 0.059679173 -0.04644315; -0.038811434 -0.00047740078 0.018940771; 0.0037847466 0.018645123 -0.018679881;;; … ;;; -0.03867359 0.047896586 -0.017179236; 0.022654168 0.01317011 0.004867884; -0.0598565 -0.034404065 0.0006101315;;; -0.01585493 0.011409832 -0.027881406; -0.036786493 -0.046842862 -0.061390653; -0.042209134 -0.0037882423 0.038311504;;; -0.046244115 -0.009695534 -0.026518239; 0.01199032 0.002633969 -0.0031427655; -0.0028447574 0.020258453 0.026971763;;;; 0.0010463276 0.025986465 -0.009244947; -0.07908115 -0.08102197 -0.008405548; 0.0102413185 0.042486306 -0.011851467;;; -0.035136916 -0.0036150715 -0.020298872; 0.04723783 -0.03883711 -0.04821234; 0.067972794 0.041136067 -0.042944588;;; 0.011533859 -0.041947626 -0.0008973482; -0.05188996 0.01169288 0.03843938; 0.050136358 -0.018973237 0.026987765;;; … ;;; 0.06441222 -0.008769459 -0.12086715; 0.00033975442 -0.016273681 -0.0034204714; 0.017421165 -0.013667475 -0.027556669;;; 0.044852618 0.05318118 0.032806322; -0.043956272 -0.034673505 -0.0648975; 0.03287062 -0.034168232 -0.031127201;;; 0.00785595 0.01782079 0.044460308; -0.005899955 -0.037252802 0.009023352; -0.0265322 -0.024867244 -0.016625555;;;; -0.04628408 0.028862204 -0.0059459023; -0.010762029 0.0011674885 -0.010457941; -0.025168713 -0.03286575 -0.038102776;;; 0.0023153275 -0.035918817 0.014513793; -0.009592184 -0.0070520337 -0.008891122; 0.014082154 -0.04479214 -0.008855094;;; 0.02380924 -0.033656914 9.650012f-5; -0.032459464 -0.050592586 0.031080868; 0.00940645 -0.023904594 0.022212608;;; … ;;; -0.06206366 -0.001785199 0.03263033; 0.042273838 -0.0315236 0.0026854551; -0.020284476 0.012975626 -0.07194324;;; -0.0035642018 -0.066399746 -0.029070158; 0.03184501 -0.015241649 -0.018779904; 0.011048754 0.0141771445 -0.042521358;;; -0.013581766 0.044455603 0.010639554; 0.02870466 -0.0006666788 -0.025261411; 0.039688826 -0.03709411 0.03343762], bias = Float32[3.1630454f-5;;; 1.8022902f-5;;; 3.2246302f-5;;; … ;;; 5.835687f-6;;; 1.3590614f-5;;; 4.7783415f-5;;;;]), layer_5 = (scale = Float32[0.99287236, 1.0540823, 1.0073037, 1.0138972, 0.9455491, 1.0144131, 0.98217684, 1.0158086, 1.0017754, 1.0200988  …  1.0374503, 1.0074041, 0.98451793, 1.0179555, 1.0349733, 1.0225339, 1.0115013, 0.99805576, 1.0085765, 1.0078199], bias = Float32[-0.049272913, -0.018671272, -0.025314134, -0.051213086, -0.1006498, -0.091665104, -0.04080951, -0.07243947, -0.033888683, -0.063454285  …  -0.011968679, 0.016927168, -0.091226935, 0.0050205677, -0.06672526, -0.07320259, 0.0070026363, -0.06812545, -0.03548137, -0.065353595]), layer_6 = NamedTuple(), layer_7 = (weight = Float32[-0.07027461;;; 0.066589676;;; -0.022998946;;; … ;;; 0.06272069;;; 0.025724504;;; 0.08988325;;;; -0.0051300586;;; -0.11097723;;; 0.03467519;;; … ;;; 0.028922008;;; 0.06851981;;; -0.04179792;;;; -0.018767586;;; -0.04859844;;; 0.014800996;;; … ;;; 0.0021075532;;; -0.08950261;;; 0.019903727;;;; … ;;;; -0.066029884;;; 0.054012004;;; 0.03286419;;; … ;;; 0.087862775;;; 0.038145367;;; 0.0077437474;;;; 0.10415296;;; -0.09054005;;; 0.008319174;;; … ;;; -0.012467136;;; -0.022936702;;; 0.09566202;;;; 0.07809948;;; -0.055788882;;; -0.1078934;;; … ;;; 0.028562954;;; 0.029193472;;; 0.09658277], bias = Float32[-4.4775872f-5;;; 8.6702436f-5;;; -0.00015359072;;; … ;;; 1.8281813f-5;;; -1.9515293f-5;;; -2.3913322f-5;;;;]), layer_8 = (scale = Float32[0.9960651, 1.0526538, 1.0013092, 1.0524077, 1.0441577, 1.0209616, 0.9758119, 1.0244993, 1.0013134, 1.0047752  …  1.0081434, 1.0293424, 1.0301474, 0.9910336, 1.0239856, 1.0449487, 1.018321, 0.9843331, 1.0072016, 1.0154037], bias = Float32[-0.012758018, -0.047629822, -0.0043830513, 0.003108096, 0.025350409, -0.008032897, -0.030731985, -0.00092974026, 0.005630054, -0.02229625  …  0.023768345, 0.0031945293, -0.009863008, -0.017409308, -0.04070011, -0.00030308054, -0.046757054, -0.0061204964, -0.0014437395, -0.01278656])), gap = NamedTuple(), conv1 = (weight = Float32[0.061334305;;; 0.093942344;;; 0.0011512325;;; … ;;; 0.060789;;; -0.034388714;;; 0.0014400035;;;; -0.0066135945;;; -0.08205952;;; -0.05527896;;; … ;;; -0.023757908;;; 0.003910753;;; 0.06306869;;;; -0.08388565;;; -0.02581031;;; -0.06878397;;; … ;;; -0.09418354;;; 0.027580278;;; 0.02187315;;;; … ;;;; -0.034821123;;; -0.0107256565;;; 0.04381767;;; … ;;; -0.039374046;;; 0.14330508;;; -0.055157624;;;; 0.05574317;;; -0.03725498;;; 0.017547635;;; … ;;; 0.050620083;;; -0.0543651;;; 0.00700764;;;; -0.069833264;;; 0.009527674;;; -0.023865024;;; … ;;; 0.023626843;;; -0.1329072;;; 0.018947273], bias = Float32[0.0010369202;;; -0.030915175;;; 0.065844074;;; … ;;; 0.04958299;;; 0.02307727;;; 0.04250266;;;;]), conv2 = (weight = Float32[-0.048690345;;; -0.028114924;;; -0.074577935;;; … ;;; 0.08539309;;; -0.011759406;;; -0.005931195;;;; 0.034184847;;; -0.080103986;;; 0.06710796;;; … ;;; -0.05529441;;; 0.062410947;;; -0.022797516;;;; 0.06722171;;; -0.0023586026;;; -0.014089858;;; … ;;; -0.053876456;;; 0.030836722;;; -0.08236427;;;; … ;;;; 0.024303714;;; 0.022982812;;; 3.695651f-5;;; … ;;; -0.036701128;;; -0.00020351005;;; -0.037842426;;;; 0.05017715;;; 0.0071453927;;; -0.068864055;;; … ;;; 0.061676204;;; -0.12722647;;; -0.05220282;;;; 0.11345438;;; 0.055851873;;; -0.013251242;;; … ;;; 0.011292874;;; 0.054008223;;; 0.012765033], bias = Float32[-0.010463048;;; 0.05325845;;; -0.0005110143;;; … ;;; -0.00967812;;; 0.00020162291;;; 0.006974888;;;;]))), conv2 = (weight = Float32[0.02442348 0.004798119 0.024737766; -0.042340185 -0.025977166 8.351815f-5; -0.03566834 -0.032243453 -0.040081095;;; -0.003922156 -0.03855539 0.01019367; 0.0032482885 -0.0029236514 -0.009426619; -0.030236064 0.004858212 0.015928824;;; -0.008365654 0.004213209 -0.022274511; 0.04454262 0.052942522 0.01722115; -0.00040954977 -0.01524016 0.03253827;;; … ;;; 0.010638064 0.035912592 0.0073706596; 0.04797673 0.008842143 0.023685606; 0.012083197 0.031411856 0.018240986;;; -0.022817519 -0.001499005 -0.008190128; 0.01833059 -0.015054735 0.0073948624; -0.043357123 -0.031539783 -0.031750944;;; -0.003046096 -0.027481398 -0.013841862; 0.044006467 0.028040495 0.03695468; 0.04169289 0.020763697 0.03659885;;;; 0.080289885 0.02496776 0.03221204; -0.031956293 -0.0017890007 0.020109462; 0.014363065 0.018289806 0.05663315;;; -0.021535072 -0.0008943423 -0.012066096; 0.0025847661 -0.035853233 -0.0077931783; -0.005066849 0.015331423 -0.010062902;;; 0.014486471 0.017597752 0.022274623; 0.027781894 -0.021572094 0.006598172; 0.014344045 0.033401884 0.0027255423;;; … ;;; 0.060627904 0.025214337 0.05216322; 0.03823461 0.0051308926 -0.0121106515; 0.008716798 0.008211455 0.030310448;;; -0.024672931 -0.029304119 0.009796339; -0.031736657 -0.06360676 -0.07067454; -0.025583565 0.021833027 0.014497946;;; 0.0006332648 -0.0076693106 0.022656646; -0.0037897218 0.007868946 0.02689267; 0.034002524 0.031645026 0.027417876;;;; -0.008850281 0.020974373 0.0037015735; 0.0029108042 0.0073470837 0.037666734; -0.011919817 -0.012550879 -0.013199414;;; 0.0056285495 0.013592381 0.014939385; 0.027755192 0.010776233 0.007971124; 0.032060128 0.00621748 -0.011208728;;; 0.00028127653 0.017793851 -0.00457228; 0.032481026 0.03208234 0.02289776; -0.019942986 0.0027743075 0.0127361175;;; … ;;; 0.058449335 -0.009663149 0.008257198; 0.0055667046 0.0024012462 0.00091610284; 0.07479949 0.054503527 0.048993453;;; -0.034923457 -0.034926806 -0.004366794; 0.026433874 0.019172879 0.030147856; 0.014081799 0.010785852 0.018630017;;; 0.0013649912 -0.011861038 0.023988921; 0.039634846 0.027410999 -0.0016873461; 0.03662702 0.004921041 0.04480308;;;; … ;;;; 0.029535633 -0.00016117343 0.024100823; 0.05772358 0.06989406 0.057557315; -0.013061208 -0.022216406 -0.036328547;;; -0.011496324 0.013309032 -0.018496824; 0.03705063 0.027268661 0.004967807; -0.0064646495 -0.00022301177 -0.013914016;;; -0.015534085 0.026903218 0.02033003; -0.0034481883 0.02689849 0.023009969; -0.032848686 0.008436931 -0.0017878445;;; … ;;; 0.048531264 0.05055502 0.015489554; 0.036406428 0.035276562 0.048537377; 0.028799856 0.03755482 0.029060522;;; -0.053487092 -0.02393998 0.017964577; -0.0014153385 0.012424315 0.014513815; -0.008973735 -0.03947844 0.010672445;;; -0.028629232 -0.0025198576 0.030736223; -0.01469435 0.003687612 0.010963959; -0.012319731 -0.010631663 0.008362224;;;; -0.008478831 -0.034030348 -0.022183813; -0.007945743 0.02107119 0.0072339373; -0.0007157285 0.037730303 0.032820784;;; 0.045891237 0.0069276216 0.03240957; -0.003100312 -0.017002497 0.0036788457; -0.01953185 -0.012780706 0.0060456553;;; -0.0038866426 0.0054275626 0.030852024; 0.0005029911 0.03599684 0.039129816; -0.029921688 0.009928848 0.007213972;;; … ;;; 0.002065018 -0.0065103155 0.01842552; -0.06484818 -0.03612655 -0.00093400996; -0.04727275 -0.047687568 -0.05139841;;; 0.011356611 0.0067108865 0.0016360312; -0.006890736 -0.017257893 0.009847082; -0.024986653 -0.016617913 -0.012008125;;; -0.031918317 -0.013326936 -0.0058331178; 0.006147014 0.018028075 0.03210056; 0.049030636 0.04256051 0.036854934;;;; -0.028485341 0.0150941955 0.0053882576; 0.06856895 0.06023129 0.04503052; 0.0055029173 0.040990777 0.009647835;;; -0.028019805 -0.011500969 -0.00054748787; 0.0044951243 -0.0010347402 0.043164838; 0.0157504 -0.0007209858 0.046858866;;; -0.00868762 -0.0054184087 -0.0031884; -0.020582626 -0.016162023 -0.0009536083; 0.035351146 0.009145649 0.026678309;;; … ;;; -0.054799847 -0.03522155 -0.011291809; -0.026243422 -0.056424115 -0.043360334; -0.034907788 -0.025596466 -0.025156751;;; 0.053951308 0.013238171 0.044521682; -0.029210476 -0.034304254 -0.024888676; 0.016690357 -0.010784661 -0.0034195704;;; -0.012782532 -0.026605148 -0.011603249; 5.497543f-5 -0.02752292 -0.03114765; 0.009229661 -0.0011819011 -0.034021694], bias = Float32[-3.7985086f-5;;; 6.963113f-5;;; 2.1311458f-5;;; … ;;; -4.5920184f-5;;; 6.804327f-5;;; 0.00010411177;;;;]), norm2 = (scale = Float32[0.9572083, 1.0063933, 1.0021636, 0.95723885, 0.97545195, 0.9557684, 0.95741636, 0.9895532, 0.970011, 0.98834187  …  0.97616285, 0.9721008, 0.99983966, 0.9554155, 0.94119155, 0.9742795, 1.0141306, 0.9866297, 0.9793815, 0.9807079], bias = Float32[-0.045464054, -0.030412368, -0.044297293, -0.051971287, -0.019844862, -0.049423512, -0.033346068, -0.01185511, -0.042863894, -0.030870678  …  -0.031950176, -0.02910019, -0.019056441, -0.044119526, -0.052381866, -0.036351718, -0.01689112, -0.029286383, -0.01981059, -0.038950264]), gmp = NamedTuple()), board_encoder = NamedTuple(), combo_encoder = NamedTuple(), btb_encoder = NamedTuple(), tspin_encoder = NamedTuple(), mino_list_encoder = NamedTuple(), attention = NamedTuple(), score_net = (layer_1 = (weight = Float32[-0.03929364 -0.018091444 … -0.02739056 -0.07077261; 0.007964216 -0.0371912 … -0.006614329 0.011266654; … ; -0.050230257 -0.041548118 … 0.03530657 0.085653275; -0.043417376 0.044368424 … 0.03270738 -0.1254026], bias = Float32[0.008456621; -0.0076601785; … ; -0.038161725; 0.0005160225;;]), layer_2 = (weight = Float32[0.03276358 -0.04652118 … -0.05191951 -0.001498291; -0.06976591 0.048730724 … 0.018286508 -0.0046080668; … ; -0.038031496 0.048613228 … 0.052753538 -0.02460217; 0.027873442 -0.003165895 … 0.058013942 -0.021358443], bias = Float32[-0.0035430635; -0.008060894; … ; -0.005298615; -0.018524801;;]), layer_3 = (weight = Float32[-0.0026027227 -0.08236338 … -0.0250767 0.095473945], bias = Float32[0.006224786;;]))), (board_net = (conv1 = NamedTuple(), norm1 = (running_mean = Float32[-0.09770536, 0.14001317, -0.08224163, -0.011329369, 0.10856394, 0.01986244, -0.010158712, 0.07539056, 0.030745849, -0.058416698  …  0.10979825, 0.2150275, -0.16469802, -0.032616083, 0.022871623, 0.058241274, 0.029050287, 0.0014688094, 0.046189923, 0.054880235], running_var = Float32[0.0026713444, 0.003696745, 0.002883378, 0.0023085054, 0.001833354, 0.0024434286, 0.001063897, 0.0026716588, 0.0024051184, 0.002747055  …  0.0030422185, 0.0049679787, 0.0054931855, 0.0005591354, 0.001644363, 0.0019943896, 0.004839719, 0.0030942743, 0.0010668669, 0.0013555194], training = Val{false}()), resblocks = (layer_1 = (resblock = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.38846493, -0.17553124, -0.07636925, -0.4057774, 0.062336646, -0.187619, 0.16880187, -0.2588173, 0.27771473, -0.12498491  …  0.07452892, -0.36762196, 0.37365088, -0.3761216, -0.3532814, -0.05459521, -0.19750342, -0.02266978, -0.10391396, -0.34424442], running_var = Float32[1.7791497, 1.72901, 0.6724528, 4.005357, 0.5826764, 1.3906249, 1.4580338, 1.5486693, 1.875173, 2.9885502  …  0.68212295, 1.9393474, 1.6617892, 1.0448415, 2.775101, 1.8073121, 0.4695351, 1.8393831, 2.1912832, 1.819709], training = Val{false}()), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-0.76444566, -0.40756077, 0.12971948, -0.23835541, -0.4066744, -0.346922, 0.16698621, 0.029020488, -0.75610995, 0.17552118  …  -0.27174464, 0.013467318, 0.3257688, -0.09003833, -0.13136408, 0.12326667, -0.79427755, -0.18292734, 0.28958786, -0.6840819], running_var = Float32[2.4178777, 1.9350541, 1.7959967, 1.4494025, 1.5840782, 2.1703644, 2.1221445, 1.7562736, 2.1114175, 1.7171104  …  2.5712032, 1.9204266, 1.7520136, 1.719913, 1.838756, 1.1405044, 2.4402728, 2.665283, 2.140254, 2.3770213], training = Val{false}()), layer_6 = NamedTuple(), layer_7 = NamedTuple(), layer_8 = (running_mean = Float32[-0.12066303, -0.107705384, -0.121849924, 0.10751333, -0.16044533, -0.061895553, -0.03894219, 0.15781471, -0.14164838, -0.12464587  …  -0.025091574, 0.038184177, -0.21384552, -0.13007593, -0.09878839, -0.18817076, 0.18955736, 0.026372267, 0.12981215, -0.15575221], running_var = Float32[0.16793452, 0.2051969, 0.18604681, 0.16950072, 0.110300876, 0.19169225, 0.20042694, 0.1374, 0.17166044, 0.15997702  …  0.14775474, 0.15813649, 0.20858382, 0.12327187, 0.17806892, 0.25921687, 0.2580477, 0.11866648, 0.19689162, 0.20192492], training = Val{false}())), gap = NamedTuple(), conv1 = NamedTuple(), conv2 = NamedTuple()), layer_2 = (resblock = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.4275324, -0.039739423, -0.29331046, 0.17367516, 0.33622816, 0.114236966, -0.123262726, -0.68957365, -0.4739181, 0.289282  …  -0.09345064, -0.1944808, -0.17161858, 0.017735327, -0.909152, -0.065739706, -0.29646918, 0.20532787, -0.15579487, 0.3035679], running_var = Float32[0.90026975, 0.66198224, 1.1167965, 1.791565, 1.0155561, 0.9039301, 0.94956195, 0.96225506, 1.2468648, 0.6209732  …  0.56558496, 1.2509363, 0.54801124, 1.0219816, 1.1561652, 1.5073594, 1.9569725, 0.609969, 0.7900405, 1.3586313], training = Val{false}()), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-0.516751, -0.07678565, -0.28012654, -0.46148083, -0.05888918, -0.61863786, -0.1163873, -0.27112696, 0.27262023, -0.26449814  …  -0.068678424, -0.23014085, -0.42913598, -0.0355462, -0.08544813, -0.20769615, 0.11412669, -0.46358413, 0.6375595, -0.799971], running_var = Float32[1.12682, 1.0963489, 1.1533607, 1.4846449, 1.6437235, 1.5730875, 1.3942641, 0.96406645, 1.475671, 0.86690795  …  2.2241752, 2.2230616, 2.3302436, 1.4754566, 1.5599507, 1.2874001, 1.6998519, 1.8964744, 1.6604053, 2.6983447], training = Val{false}()), layer_6 = NamedTuple(), layer_7 = NamedTuple(), layer_8 = (running_mean = Float32[-0.048350483, 0.14061786, 0.19768848, 0.10999956, 0.049032442, -0.058022577, -0.11496245, 0.085486434, 0.030145368, -0.06917214  …  0.23446165, -0.02592244, 0.048170358, -0.06336993, -0.22818689, -0.23754816, -0.07643326, -0.0167473, -0.05059225, 0.059343442], running_var = Float32[0.15428388, 0.18223426, 0.213553, 0.1855577, 0.11397497, 0.16304223, 0.15175325, 0.23993286, 0.29630372, 0.13173491  …  0.29685006, 0.22082563, 0.20267378, 0.15216418, 0.1409303, 0.3216911, 0.24511184, 0.1291801, 0.23822471, 0.21994294], training = Val{false}())), gap = NamedTuple(), conv1 = NamedTuple(), conv2 = NamedTuple()), layer_3 = (resblock = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.19338816, -0.059785224, 0.37579805, -0.45206848, 0.498374, -0.45172083, -0.25492, 0.41028756, -0.060832176, -0.50270516  …  -0.17815149, -0.14914678, 0.11245117, -0.35199663, -0.22846532, 0.10744371, 0.38238743, 0.5717073, -0.3332244, -0.39102253], running_var = Float32[1.2108874, 0.7776442, 0.7599885, 0.9140827, 0.69751877, 1.1594837, 0.837025, 0.6461775, 0.6600941, 0.55535215  …  0.7642581, 0.9469556, 0.8888014, 1.0271864, 1.1435611, 0.7616357, 0.80726224, 0.79241276, 0.73654145, 1.1273862], training = Val{false}()), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-0.20285583, -0.021496078, -0.18564841, -0.71186167, -0.14054772, 0.07676284, -0.034638885, -0.1661612, 0.28722286, -0.7652208  …  -0.39055797, -0.14373013, -0.123230405, -0.61435723, -0.39833114, -0.52319735, -0.0009949867, -0.46561852, 0.17136285, 0.049621545], running_var = Float32[1.002792, 0.81070614, 0.8686418, 1.3460834, 0.6992142, 1.1610223, 0.7927665, 0.85206693, 0.91331583, 0.91776216  …  1.1424631, 1.1597929, 0.99398094, 1.3399625, 0.8183631, 1.1686419, 1.1070123, 1.2895721, 0.92210054, 0.9503147], training = Val{false}()), layer_6 = NamedTuple(), layer_7 = NamedTuple(), layer_8 = (running_mean = Float32[-0.05586127, 0.09320006, 0.055887576, 0.010249587, -0.20699449, -0.1360948, 0.005068883, 0.280219, 0.09283036, -0.21023695  …  -0.005599961, -0.08757274, 0.104335666, 0.26146173, -0.2072271, -0.16874161, -0.14009179, -0.19872956, 0.10518073, -0.17129166], running_var = Float32[0.21275322, 0.27111953, 0.16486332, 0.20788859, 0.26929933, 0.15092824, 0.1435143, 0.38342524, 0.303739, 0.1362308  …  0.15637413, 0.14115486, 0.21048775, 0.3288593, 0.18387675, 0.20624264, 0.20591258, 0.16550526, 0.18453224, 0.14774044], training = Val{false}())), gap = NamedTuple(), conv1 = NamedTuple(), conv2 = NamedTuple()), layer_4 = (resblock = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.106949285, -0.29728854, -0.0794971, -0.36848274, -0.7163801, -0.35210264, -0.5323473, -0.15424666, -0.040900555, -0.51473075  …  -0.33271697, 0.097161025, 0.3195851, 0.14204487, -0.19494449, 0.1154831, -0.046275314, -0.10785675, 0.10993172, 0.17910945], running_var = Float32[0.5348385, 1.3325121, 1.0089287, 0.97940207, 1.709023, 0.98965985, 0.868335, 1.0457467, 1.1777073, 1.2602253  …  1.0625384, 0.9102396, 1.0593351, 0.6746117, 1.3814837, 0.81036615, 0.79455787, 0.9626329, 1.3713492, 1.020532], training = Val{false}()), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[0.14328456, 0.040455133, -0.17351823, -0.7710013, -0.1095569, 0.4969684, 0.063265845, -0.3421619, 0.025033299, 0.2723588  …  -0.10850291, -0.10730969, 0.038921516, -0.7763119, -0.03259133, -0.20040235, 0.8718438, 0.0464144, 0.30162323, 0.060823243], running_var = Float32[1.0939147, 1.4251662, 2.020759, 1.5231141, 1.0580776, 1.0126301, 1.3353882, 1.6059626, 1.2032243, 2.2089453  …  1.7585462, 1.222364, 1.5806583, 2.186418, 1.4258921, 1.3146352, 1.7874938, 1.5499723, 0.9549269, 1.334398], training = Val{false}()), layer_6 = NamedTuple(), layer_7 = NamedTuple(), layer_8 = (running_mean = Float32[-0.10224929, -0.03039981, 0.060000204, 0.23361532, 0.053077426, -0.122327164, -0.007788136, 0.1626588, 0.24675666, 0.049627453  …  0.34546846, 0.12763514, 0.25465477, -0.08549892, 0.042435903, -0.16130668, -0.08986745, -0.1397497, 0.15642825, -0.11396277], running_var = Float32[0.14839774, 0.26524428, 0.1513807, 0.17016529, 0.19504385, 0.1799054, 0.19548751, 0.26654866, 0.17469598, 0.20075215  …  0.15462844, 0.1847943, 0.111412525, 0.17915544, 0.26915932, 0.12906106, 0.22059987, 0.12730193, 0.20621845, 0.13623555], training = Val{false}())), gap = NamedTuple(), conv1 = NamedTuple(), conv2 = NamedTuple()), layer_5 = (resblock = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.19035456, 0.0121486215, 1.4723215, 0.3852151, -0.1184789, 0.4374936, 0.055818964, 0.3020007, -0.5208281, 0.16421732  …  -0.4382682, -0.081731446, 1.1238186, 0.2736144, 0.1406049, 0.3982391, 0.1083965, -0.019503789, -0.02340684, -0.020587116], running_var = Float32[0.8745575, 1.4752905, 1.666924, 1.3710481, 1.3114399, 1.4163905, 0.94634336, 1.4619316, 1.1800307, 1.1692532  …  1.3425866, 0.9857222, 2.0282438, 1.2416129, 1.6568537, 1.2410738, 1.1192223, 1.24736, 1.060135, 1.4569975], training = Val{false}()), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-0.4615799, -0.07409064, -0.12667164, -0.39117238, 0.04323977, 0.12959515, 1.950648, -0.35259524, -0.66144353, -0.33404437  …  -1.0695614, -1.0173693, 0.38199195, -0.95623773, 0.2522746, 0.041169006, -1.3682797, 0.00971148, -1.1296796, 0.23120476], running_var = Float32[1.8767973, 1.2844801, 1.880444, 4.942853, 3.1029398, 2.9919837, 3.6540995, 2.3659384, 2.6919546, 2.8704882  …  1.1724056, 2.2427268, 4.784462, 1.6997669, 1.0932275, 3.7445784, 4.170565, 2.7828746, 1.8070323, 1.9506103], training = Val{false}()), layer_6 = NamedTuple(), layer_7 = NamedTuple(), layer_8 = (running_mean = Float32[-0.031336717, -0.03867374, -0.04910208, 0.3313903, -0.2526239, -0.22814706, -0.09717223, 0.08214607, 0.19062923, -0.07553934  …  0.122147165, -0.31575426, 0.00668969, -0.25309202, 0.1585404, -0.24400586, 0.19768077, -0.1802666, -0.058325943, -0.17837012], running_var = Float32[0.37329534, 0.19026215, 0.49043283, 0.4620684, 0.12628269, 0.23501217, 0.12825626, 0.17755663, 0.32686096, 0.29840696  …  0.31328934, 0.2335127, 0.19125229, 0.80416405, 0.10601203, 0.22533578, 0.33843663, 0.3864925, 0.115331575, 0.16949835], training = Val{false}())), gap = NamedTuple(), conv1 = NamedTuple(), conv2 = NamedTuple())), conv2 = NamedTuple(), norm2 = (running_mean = Float32[-2.3554552, -0.6254702, 0.7273154, -1.7085205, 1.131817, -4.32753, 1.6626824, 0.23537815, -2.571205, -0.63816094  …  0.8396746, -0.18846321, 2.493036, -0.9012116, -1.7818277, 1.5366865, 0.3593417, 1.1745857, 1.1014001, -1.3684795], running_var = Float32[75.40714, 36.297523, 62.332233, 61.04527, 74.2228, 92.60545, 112.67381, 44.78759, 65.2212, 15.56018  …  35.00457, 69.24951, 12.887467, 48.334976, 82.30871, 30.518755, 46.427677, 85.448494, 97.729965, 52.390343], training = Val{false}()), gmp = NamedTuple()), board_encoder = NamedTuple(), combo_encoder = NamedTuple(), btb_encoder = NamedTuple(), tspin_encoder = NamedTuple(), mino_list_encoder = NamedTuple(), attention = NamedTuple(), score_net = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, ps, st = loadmodel(\"play/mainmodel.jld2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24x10\n",
    "board = [\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    1 1 1 1 0 0 0 0 0 0\n",
    "    1 1 1 1 1 0 0 0 0 0\n",
    "    1 1 1 1 1 1 0 0 0 0 \n",
    "    1 1 1 1 1 1 1 0 1 0 \n",
    "    1 1 1 1 1 1 1 1 1 0 \n",
    "    1 1 1 1 1 1 1 1 1 0    \n",
    "]\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24x10\n",
    "minopos = [\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    1 1 0 0 0 0 0 0 0 0\n",
    "    1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "    0 0 0 0 0 0 0 0 0 0\n",
    "]\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8910629166050172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8947638158954957\n",
      "0.8876966116067451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8867214969132191\n",
      "0.89439129045886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8974967332106016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9100923726458477\n"
     ]
    }
   ],
   "source": [
    "for i in 1:7\n",
    "    combo = 0\n",
    "    btb = 0\n",
    "    tspin = 0\n",
    "    # [i_mino, o_mino, s_mino, z_mino, j_mino, l_mino, t_mino]\n",
    "    holdnext = zeros(7, 6)\n",
    "    holdnext[1, 1] = 1\n",
    "    holdnext[6, 2] = 1\n",
    "    holdnext[6, 3] = 1\n",
    "    holdnext[6, 4] = 1\n",
    "    holdnext[6, 5] = 1\n",
    "    holdnext[i, 6] = 1\n",
    "    board_array = reshape(board, (24, 10, 1, 1))\n",
    "    minopos_array = reshape(minopos, (24, 10, 1, 1))\n",
    "    combo_array = reshape([combo], (1, 1))\n",
    "    btb_array = reshape([btb], (1, 1))\n",
    "    tspin_array = reshape([tspin], (1, 1))\n",
    "    holdnext_array = reshape(holdnext, (7, 6, 1))\n",
    "    r, _ = model((board_array, minopos_array, combo_array, btb_array, tspin_array, holdnext_array), ps, st)\n",
    "    println(r[1])\n",
    "end\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6434566091194045;;], (board_net = (conv1 = NamedTuple(), norm1 = (running_mean = Float32[0.14974095, -0.21898468, -0.014769718, -0.1503807, 0.026078673, -0.024106713, -0.24083887, 0.059361987, -0.003845987, 0.41178003  …  0.045489825, 0.030363152, -0.20738648, -0.020222384, 0.00031150825, -0.0016922948, 0.067168884, 0.0981814, 0.21098882, 0.025728004], running_var = Float32[0.0069063716, 0.008884894, 0.0059582987, 0.0027779972, 0.006650655, 0.004324917, 0.0054720887, 0.0037068252, 0.002145184, 0.00937495  …  0.004223671, 0.0010258047, 0.014236818, 0.0013360955, 0.0061511416, 0.004227165, 0.004984162, 0.0025622132, 0.006768717, 0.00062085147], training = Val{false}()), resblocks = (layer_1 = (resblock = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.08709227, -0.012833809, 0.27739304, -0.24877937, 0.021279017, -0.12407305, 0.22637543, 0.09612584, 0.11307646, 0.17361048  …  -0.31004447, -0.17861027, -0.035727594, 0.36581355, 0.16892284, -0.100335374, -0.089076325, -0.051944993, 0.2556526, 0.49805835], running_var = Float32[0.2680764, 0.36873996, 0.39845374, 0.25704592, 0.5272686, 0.4806581, 0.24580452, 0.46556568, 0.23189688, 0.28013003  …  0.34543243, 0.34206405, 0.41837242, 0.49868524, 0.7523095, 0.5295163, 0.33686104, 0.63267744, 0.3221233, 0.38398758], training = Val{false}()), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-0.31644154, 0.13668065, -0.0664634, 0.32415384, 0.3368771, -0.27086195, -0.25661084, -0.13526872, -0.35935965, -0.19742417  …  -0.38097054, 0.5421955, 0.08741591, 0.18060534, -0.24752878, 0.10564204, -0.05743978, 0.10191942, 0.3078394, -0.27473158], running_var = Float32[0.22098188, 0.4749413, 0.35432833, 0.4166629, 0.29645064, 0.4032893, 0.5174844, 0.87742954, 0.6764064, 0.46395564  …  0.9430175, 0.44598833, 0.28761923, 0.42725688, 0.3699461, 0.5031648, 0.25774655, 0.35285977, 0.36205462, 0.46624735], training = Val{false}()), layer_6 = NamedTuple(), layer_7 = NamedTuple(), layer_8 = (running_mean = Float32[0.26134956, 0.106976695, -0.030905662, -0.11437334, -0.1544914, -0.0034446695, -0.053274054, -0.08543906, -0.19424284, 0.048791494  …  -0.12649351, 0.17178984, -0.1331139, 0.028547116, 0.035090227, -0.025719428, -0.06370492, 0.03583113, 0.1047268, -0.037316304], running_var = Float32[0.047290567, 0.128886, 0.09486837, 0.12917854, 0.088435, 0.08810101, 0.08859251, 0.056133498, 0.07951756, 0.0885669  …  0.09067115, 0.10000338, 0.090271845, 0.1564712, 0.06104192, 0.051621493, 0.11483647, 0.08350884, 0.16561846, 0.07117677], training = Val{false}())), gap = NamedTuple(), conv1 = NamedTuple(), conv2 = NamedTuple()), layer_2 = (resblock = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.49210387, -0.045663673, 0.4934703, 0.015525588, 0.1390012, 0.32540873, -0.4094344, -0.039565217, -0.39049107, 0.23486803  …  0.2292955, -0.045135707, -0.06582442, -0.033523437, -0.22660871, -0.051866114, -0.066699095, -0.09263775, -0.13638152, -0.00793272], running_var = Float32[0.6117433, 0.23817855, 0.35721987, 0.45109874, 0.2344126, 0.25176546, 0.29764196, 0.38252297, 0.31069422, 0.4025048  …  0.21289454, 0.24992721, 0.3880363, 0.30479178, 0.3087516, 0.6157406, 0.37425837, 0.27943286, 0.18175736, 0.28616762], training = Val{false}()), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[-0.08278748, -0.23900872, 0.05286342, -0.07588199, -0.17434703, -0.02605018, 0.2792845, -0.103363186, -0.22422022, -0.1478576  …  -0.08448512, -0.13547085, -0.23801279, -0.04541243, 0.2932925, -0.3241756, -0.3101377, 0.2363409, -0.37813273, -0.04635883], running_var = Float32[0.35006452, 0.38821667, 0.30407894, 0.24959067, 0.281654, 0.406596, 0.55024576, 0.47515023, 0.54947025, 0.31155887  …  0.24270192, 0.30373362, 0.45931968, 0.3490812, 0.47388652, 0.5794782, 0.3343375, 0.31851974, 0.6154181, 0.3291153], training = Val{false}()), layer_6 = NamedTuple(), layer_7 = NamedTuple(), layer_8 = (running_mean = Float32[0.0141883455, 0.2029866, 0.09346515, -0.105824575, -0.027061077, -0.15081145, -0.01016756, 0.061629903, 0.14909074, 0.08590982  …  0.010669331, -0.060786035, -0.11616137, -0.13200943, -0.0233738, 0.13104974, -0.22045803, -0.0064830366, -0.065953106, -0.03117074], running_var = Float32[0.06826104, 0.10921938, 0.1031811, 0.05648662, 0.0448368, 0.0890748, 0.09029413, 0.052273043, 0.08413388, 0.11471121  …  0.103793144, 0.058474764, 0.07230694, 0.08551323, 0.11015179, 0.08169955, 0.0680694, 0.07361151, 0.059514847, 0.071550965], training = Val{false}())), gap = NamedTuple(), conv1 = NamedTuple(), conv2 = NamedTuple()), layer_3 = (resblock = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.10418142, -0.16406122, 0.039421838, -0.18287265, 0.017677363, 0.08042434, -0.30182597, 0.31589356, -0.43718815, -0.15925078  …  -0.10354364, 0.29420346, 0.2764114, -0.057187527, -0.29436976, -0.040642492, -0.107449315, -0.12405552, -0.44939047, 0.11676677], running_var = Float32[0.24928138, 0.22806406, 0.2065391, 0.2357335, 0.40276474, 0.36806113, 0.29073817, 0.26786193, 0.29613185, 0.30566475  …  0.46334988, 0.46017274, 0.4263273, 0.34959853, 0.21884088, 0.27790678, 0.2978239, 0.4124886, 0.31375673, 0.2965076], training = Val{false}()), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[0.12712762, 0.06531068, -0.31469357, 0.22002122, -0.13102727, -0.19094297, -0.1444837, -0.06279043, -0.025304109, 0.107639454  …  -0.084869705, -0.13449442, -0.0029847424, 0.4063355, 0.29516822, 0.08361549, -0.11564341, -0.27700603, -0.14831652, -0.59574753], running_var = Float32[0.3463695, 0.4946771, 0.34751508, 0.48580033, 0.23399688, 0.25774813, 0.4386965, 0.2775419, 0.33405077, 0.5095514  …  0.26014632, 0.30402708, 0.28914288, 0.40781343, 0.40904152, 0.28135026, 0.36527222, 0.34454575, 0.25259006, 0.5771488], training = Val{false}()), layer_6 = NamedTuple(), layer_7 = NamedTuple(), layer_8 = (running_mean = Float32[-0.28379232, 0.0406025, 0.030650666, 0.02812956, 0.13467324, 0.2629818, -0.0044512576, -0.007798433, 0.034019083, -0.09342192  …  0.020368023, -0.04677955, 0.01829214, 0.07589685, 0.100426346, 0.141798, -0.1758901, 0.0034616834, -0.07994925, 0.05746176], running_var = Float32[0.13599716, 0.069038935, 0.16109076, 0.0705835, 0.09620314, 0.13366431, 0.11230135, 0.07651405, 0.05243586, 0.107878216  …  0.05768567, 0.10842577, 0.08506663, 0.09246444, 0.09043652, 0.10844024, 0.119228534, 0.07243636, 0.10339268, 0.13110566], training = Val{false}())), gap = NamedTuple(), conv1 = NamedTuple(), conv2 = NamedTuple()), layer_4 = (resblock = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0010452126, -0.15294719, 0.025628917, 0.07163963, -0.5373363, -0.105652854, -0.41780686, 0.15119623, 0.14231911, 0.14983507  …  0.24486548, -0.39866674, -0.20423178, 0.07400919, -0.011065469, -0.30906722, 0.37903872, -0.47568524, -0.12177776, -0.23157144], running_var = Float32[0.40768242, 0.22195473, 0.38945186, 0.2845466, 0.394083, 0.31931856, 0.37830025, 0.17608574, 0.27739492, 0.42091468  …  0.3091878, 0.36190537, 0.3028638, 0.25667745, 0.33654532, 0.3292749, 0.27866143, 0.43177938, 0.4027107, 0.33354062], training = Val{false}()), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[0.3228083, 0.15552096, 0.0949997, -0.17933543, 0.1731253, 0.47351494, 0.017318128, -0.53939587, -0.09872355, 0.042338494  …  -0.0045353156, -0.13789709, 0.07139903, -0.020547079, -0.16803446, 0.39679638, -0.12639998, 0.07365965, -0.097330295, -0.29828942], running_var = Float32[0.3227915, 0.2859549, 0.36841956, 0.2760009, 0.2846669, 0.35067174, 0.4087969, 0.6328435, 0.35702965, 0.3007502  …  0.32834622, 0.22810155, 0.42558336, 0.3163987, 0.4141882, 0.34670997, 0.5038682, 0.2786101, 0.4573942, 0.73211426], training = Val{false}()), layer_6 = NamedTuple(), layer_7 = NamedTuple(), layer_8 = (running_mean = Float32[0.071496405, 0.23055546, 0.16022141, 0.014259818, -0.080398075, 0.009296401, 0.11393131, -0.16758226, -0.02595341, 0.016009893  …  -0.013078879, -0.026975634, -0.17447278, -0.03632232, -0.25574017, 0.05532416, 0.0067526395, -0.077646926, 0.043337025, 0.15680529], running_var = Float32[0.082313076, 0.1723403, 0.06372498, 0.057433173, 0.0694909, 0.058041025, 0.14787857, 0.155272, 0.08785571, 0.09511714  …  0.11193069, 0.0806764, 0.201281, 0.12303565, 0.10195538, 0.12301992, 0.06921514, 0.12083807, 0.08419222, 0.100915335], training = Val{false}())), gap = NamedTuple(), conv1 = NamedTuple(), conv2 = NamedTuple()), layer_5 = (resblock = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[-0.05581576, -0.106982365, -0.28098482, 0.2950761, 0.047059298, 0.10087222, -0.31192318, -0.025692038, -0.027721163, -0.19149092  …  -0.18838882, 0.076620474, 0.17797692, 0.012608514, 0.09115244, 0.3517651, -0.31230026, 0.12455891, -0.36908522, 0.36059883], running_var = Float32[0.4774408, 0.22920777, 0.31722942, 0.3816769, 0.2720042, 0.30416426, 0.5780861, 0.21390219, 0.54516655, 0.25051874  …  0.3162802, 0.33947825, 0.2956072, 0.28397787, 0.4871592, 0.3930451, 0.58939, 0.59722024, 0.43319896, 0.4043828], training = Val{false}()), layer_3 = NamedTuple(), layer_4 = NamedTuple(), layer_5 = (running_mean = Float32[0.15824516, 0.15760778, 0.20947976, -0.057084147, -0.103242904, -0.09897724, 0.25966728, -0.5111643, 0.266596, 0.2154302  …  0.045495577, -0.1681069, -0.22750098, 0.05703637, -0.09634519, 0.057408724, -0.3365201, -0.05200265, -0.09241217, -0.020631168], running_var = Float32[0.6566225, 0.25363234, 0.37229252, 0.3751218, 0.31000635, 0.43743673, 0.38206327, 0.5992906, 0.3352927, 0.3427608  …  0.29145604, 0.62869114, 0.4485873, 0.4611525, 0.3300392, 0.48671666, 0.64113045, 0.27028102, 0.29488444, 0.3369458], training = Val{false}()), layer_6 = NamedTuple(), layer_7 = NamedTuple(), layer_8 = (running_mean = Float32[0.07670317, -0.058677055, -0.028187504, 0.011467326, 0.1098523, 0.1731618, -0.034712378, 0.053991307, 0.057033237, 0.08634686  …  0.06935113, 0.13811572, -0.06236612, 0.07935031, 0.17806728, -0.04494405, 0.0831231, 0.049155287, 0.0043735136, -0.09384539], running_var = Float32[0.05585005, 0.1195967, 0.08603925, 0.12070152, 0.1884141, 0.16054961, 0.06888886, 0.1349494, 0.16787246, 0.20281522  …  0.056782126, 0.26638696, 0.09845659, 0.0749391, 0.13933358, 0.06018502, 0.14755344, 0.047164552, 0.06538687, 0.06917819], training = Val{false}())), gap = NamedTuple(), conv1 = NamedTuple(), conv2 = NamedTuple())), conv2 = NamedTuple(), norm2 = (running_mean = Float32[0.262063, 0.3301243, -0.81832653, 0.40700412, -0.06628724, -0.24056129, -0.37536997, 0.24815077, 0.4539897, -0.93728983  …  0.29860285, -0.67470956, 0.38012215, -0.28499943, 0.08245787, 0.042841457, -0.026466632, -0.6033175, 0.30657342, -0.5288463], running_var = Float32[2.0076134, 3.0305119, 1.3526434, 2.2245178, 1.2556655, 2.459345, 2.5090263, 0.6567313, 2.8969154, 2.36918  …  1.8555428, 2.3350384, 2.5382361, 2.7771187, 3.6301646, 1.3636416, 3.122459, 2.5478072, 3.3909068, 3.5790577], training = Val{false}()), gmp = NamedTuple()), board_encoder = NamedTuple(), combo_encoder = NamedTuple(), btb_encoder = NamedTuple(), tspin_encoder = NamedTuple(), mino_list_encoder = NamedTuple(), attention = (embedding = NamedTuple(), positional_encoding = (pos_enc = Float32[0.7617204 0.98704624 … -0.9277093 -0.88542116; 0.731761 0.07094825 … -0.8208616 -0.21141624; … ; 1.154782f-8 2.309564f-8 … 5.77391f-8 6.928692f-8; 1.0 1.0 … 1.0 1.0;;;],), blocks = (layer_1 = (mha = (query = NamedTuple(), key = NamedTuple(), value = NamedTuple(), dropout = NamedTuple(), output = NamedTuple()), layer_norm1 = NamedTuple(), ffn = (layer_1 = NamedTuple(), layer_2 = NamedTuple()), dropout = NamedTuple(), layer_norm2 = NamedTuple()), layer_2 = (mha = (query = NamedTuple(), key = NamedTuple(), value = NamedTuple(), dropout = NamedTuple(), output = NamedTuple()), layer_norm1 = NamedTuple(), ffn = (layer_1 = NamedTuple(), layer_2 = NamedTuple()), dropout = NamedTuple(), layer_norm2 = NamedTuple())), output = (layer_1 = NamedTuple(), layer_2 = NamedTuple())), score_net = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model((board_array, minopos_array, combo_array, btb_array, tspin_array, holdnext_array) , ps , st )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NNlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat3(args...) = cat(args..., dims=3)\n",
    "neg(x::AbstractArray{T}) where {T} = convert(T, -1.0) * x .+ convert(T, 1.0)\n",
    "\n",
    "z = cat3(neg(board_array),neg(minopos_array))\n",
    "z, _ = model.board_net.conv1(z, ps.board_net.conv1, st.board_net.conv1)\n",
    "z, _ = model.board_net.norm1(z, ps.board_net.norm1, st.board_net.norm1)\n",
    "z = swish(z)\n",
    "for i in 1:15\n",
    "z, _ = model.board_net.resblocks[i](\n",
    "    z, ps.board_net.resblocks[i], st.board_net.resblocks[i]\n",
    ")\n",
    "end\n",
    "\n",
    "# z, _ = model.board_net.conv2(z, ps.board_net.conv2, st.board_net.conv2)\n",
    "# z, st_norm2 = model.board_net.norm2(z, ps.board_net.norm2, st.board_net.norm2)\n",
    "# z = swish(z)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tspinz = z\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tspinz - z\n",
    "save_3d_array_as_grid_with_padding(d[:, :, :, 1], \"conv1.png\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.board_net.resblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0-rc1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0-rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
